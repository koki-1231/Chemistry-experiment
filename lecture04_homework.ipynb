{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koki-1231/Chemistry-experiment/blob/main/lecture04_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUWcdth_khfN"
      },
      "source": [
        "# 第4回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAjuP7I4lWyn"
      },
      "source": [
        "## 課題\n",
        "\n",
        "今Lessonで学んだことを元に，MNISTのファッション版 (Fashion MNIST，クラス数10) を多層パーセプトロンによって分類してみましょう．\n",
        "\n",
        "Fashion MNISTの詳細については以下のリンクを参考にしてください．\n",
        "\n",
        "Fashion MNIST: https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpiz19GRlZ_9"
      },
      "source": [
        "### 目標値\n",
        "\n",
        "Accuracy 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSHeI_utleEN"
      },
      "source": [
        "### ルール\n",
        "\n",
        "\n",
        "- 訓練データは`x_train`， `t_train`，テストデータは`x_test`で与えられます．\n",
        "- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください．\n",
        "- **下のセルで指定されている`x_train`，`t_train`以外の学習データは使わないでください．**\n",
        "- Pytorchを利用して構いません．\n",
        "- ただし，**`torch.nn.Conv2d`のような高レベルのAPIは使用しないで下さい**．具体的には，`nn.Parameter`, `nn.Module`, `nn.Sequential`以外の`nn`系のAPIです．使用した場合エラーになります．\n",
        "- torchvision等で既に実装されているモデルも使用しないで下さい．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diuec-_YluI6"
      },
      "source": [
        "### 提出方法\n",
        "- 2つのファイルを提出していただきます．\n",
        "    1. テストデータ (`x_test`) に対する予測ラベルをcsv形式で保存し，**Omnicampusの宿題タブから「第4回 ニューラルネットワークの最適化・正則化」を選択して**提出してください．\n",
        "    2. それに対応するpythonのコードを　ファイル＞ダウンロード＞.pyをダウンロード　から保存し，**Omnicampusの宿題タブから「第4回 ニューラルネットワークの最適化・正則化 (code)」を選択して**提出してください．pythonファイル自体の提出ではなく，「提出内容」の部分にコード全体をコピー&ペーストしてください．\n",
        "      \n",
        "- なお，採点は1で行い，2はコードの確認用として利用します（成績優秀者はコード内容を公開させていただくかもしれません）．コードの内容を変更した場合は，**1と2の両方を提出し直してください**．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hofSzJsVlvKp"
      },
      "source": [
        "### 評価方法\n",
        "- 予測ラベルの`t_test`に対する精度 (Accuracy) で評価します．\n",
        "- 即時採点しLeader Boardを更新します（採点スケジュールは別アナウンス）．\n",
        "- 締切時の点数を最終的な評価とします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYtPC5OjKFM"
      },
      "source": [
        "### ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "efPoZs_KjKFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d73681-87bc-4c9c-fccc-118dff26b23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 作業ディレクトリを指定\n",
        "work_dir = 'drive/MyDrive/Colab Notebooks/DLBasics2025_colab'"
      ],
      "metadata": {
        "id": "jZI4dfoQdaRM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu4cmQtelx19"
      },
      "source": [
        "### データの読み込み（この部分は修正しないでください）\n",
        "\n",
        "`__len__`は，Pythonの組み込み関数len()を呼んだときに，内部で呼ばれる特殊メソッドです．\n",
        "\n",
        "`__getitem__`は，インデックスやキーで要素を取得するときに，内部で呼ばれる特殊メソッドです．\n",
        "\n",
        "どちらも， Datasetクラスを自作する際によく登場します．\n",
        "\n",
        "```python\n",
        "class MyList:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "mylist = MyList([10, 20, 30])\n",
        "print(len(mylist))  # __len__が呼び出される\n",
        "# 3\n",
        "print(mylist[1])  # __getitem__が呼び出される\n",
        "# 20\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EsLDDSUJkRx-"
      },
      "outputs": [],
      "source": [
        "# API制限のコードと，torchに統合されているdynamoライブラリの挙動がコンフリクトを起こすため，dynamoを無効化\n",
        "import torch._dynamo\n",
        "torch._dynamo.disable()\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import inspect\n",
        "\n",
        "# 使用できるAPIを制限\n",
        "nn_except = [\"Module\", \"Parameter\", \"Sequential\"]\n",
        "for m in inspect.getmembers(nn):\n",
        "    if not m[0] in nn_except and m[0][0:2] != \"__\":\n",
        "        delattr(nn, m[0])\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習データ\n",
        "x_train = np.load(work_dir + '/Lecture04/data/x_train.npy')\n",
        "t_train = np.load(work_dir + '/Lecture04/data/y_train.npy')\n",
        "\n",
        "# テストデータ\n",
        "x_test = np.load(work_dir + '/Lecture04/data/x_test.npy')\n",
        "\n",
        "class train_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_train, t_train):\n",
        "        self.x_train = x_train.reshape(-1, 784).astype('float32') / 255\n",
        "        self.t_train = t_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x_train[idx], dtype=torch.float), torch.tensor(self.t_train[idx], dtype=torch.long)\n",
        "\n",
        "class test_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_test):\n",
        "        self.x_test = x_test.reshape(-1, 784).astype('float32') / 255\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_test.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x_test[idx], dtype=torch.float)\n",
        "\n",
        "trainval_data = train_dataset(x_train, t_train)\n",
        "test_data = test_dataset(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrSpHDIWOfK_"
      },
      "source": [
        "### 多層パーセプトロンの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKAe0F36nSvU"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "val_size = 10000\n",
        "train_size = len(trainval_data) - val_size\n",
        "\n",
        "train_data, val_data = torch.utils.data.random_split(trainval_data, [train_size, val_size])\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    val_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PADQiKNa2snb"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    # WRITE ME\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    # WRITE ME\n",
        "\n",
        "\n",
        "class Dense(nn.Module):  # nn.Moduleを継承する\n",
        "    # WRITE ME\n",
        "\n",
        "\n",
        "class MLP(nn.Module):  # nn.Moduleを継承する\n",
        "    # WRITE ME\n",
        "\n",
        "in_dim = 784\n",
        "hid_dim = 200\n",
        "out_dim = 10\n",
        "lr = 0.001\n",
        "n_epochs = 10\n",
        "\n",
        "\n",
        "mlp = # WRITE ME\n",
        "\n",
        "optimizer = # WRITE ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlOZuLu-328i"
      },
      "outputs": [],
      "source": [
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    train_num = 0\n",
        "    train_true_num = 0\n",
        "    valid_num = 0\n",
        "    valid_true_num = 0\n",
        "\n",
        "    mlp.train()  # 訓練時には勾配を計算するtrainモードにする\n",
        "    for x, t in dataloader_train:\n",
        "        # WRITE ME\n",
        "\n",
        "        losses_train.append(loss.tolist())\n",
        "\n",
        "        acc = torch.where(t - pred.to(\"cpu\") == 0, torch.ones_like(t), torch.zeros_like(t))\n",
        "        train_num += acc.size()[0]\n",
        "        train_true_num += acc.sum().item()\n",
        "\n",
        "    mlp.eval()  # 評価時には勾配を計算しないevalモードにする\n",
        "    for x, t in dataloader_valid:\n",
        "        # WRITE ME\n",
        "\n",
        "        losses_valid.append(loss.tolist())\n",
        "\n",
        "        acc = torch.where(t - pred.to(\"cpu\") == 0, torch.ones_like(t), torch.zeros_like(t))\n",
        "        valid_num += acc.size()[0]\n",
        "        valid_true_num += acc.sum().item()\n",
        "\n",
        "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n",
        "        epoch,\n",
        "        np.mean(losses_train),\n",
        "        train_true_num/train_num,\n",
        "        np.mean(losses_valid),\n",
        "        valid_true_num/valid_num\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq3scS5j4Rt2"
      },
      "outputs": [],
      "source": [
        "mlp.eval()\n",
        "\n",
        "t_pred = []\n",
        "for x in dataloader_test:\n",
        "\n",
        "    x = x.to(device)\n",
        "\n",
        "    # 順伝播\n",
        "    y = mlp.forward(x)\n",
        "\n",
        "    # モデルの出力を予測値のスカラーに変換\n",
        "    pred = y.argmax(1).tolist()\n",
        "\n",
        "    t_pred.extend(pred)\n",
        "\n",
        "submission = pd.Series(t_pred, name='label')\n",
        "submission.to_csv(work_dir + '/Lecture04/submission_pred_04.csv', header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import inspect\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ===================================================================\n",
        "# 0. 環境設定とAPI制限 (★改善点 1: 「コード1」の安全なAPI制限を導入)\n",
        "# ===================================================================\n",
        "\n",
        "# --- Google Driveマウントと作業ディレクトリ設定 ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # ★★★ ご自身の環境に合わせて修正してください ★★★\n",
        "    work_dir = '/content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab'\n",
        "    if not os.path.exists(work_dir):\n",
        "        print(f\"警告: ディレクトリ '{work_dir}' が存在しません。'.' を使用します。\")\n",
        "        work_dir = '.'\n",
        "except ImportError:\n",
        "    print(\"Google Colab 環境ではありません。カレントディレクトリ '.' を使用します。\")\n",
        "    work_dir = '.'\n",
        "print(f\"作業ディレクトリ: {work_dir}\")\n",
        "\n",
        "\n",
        "# ★★★ 改善点 1: 「コード1」で使われていた安全なAPI制限に変更 ★★★\n",
        "# optim.Adamが依存する nn.functional や nn.attention などの\n",
        "# 内部モジュールを削除しないようにします。\n",
        "nn_except = [\"Module\", \"Parameter\", \"Sequential\", \"parameter\", \"init\", \"CrossEntropyLoss\", \"ModuleList\"] # Added ModuleList\n",
        "\n",
        "for m in inspect.getmembers(nn):\n",
        "    attr_name = m[0]\n",
        "    attr_value = m[1]\n",
        "\n",
        "    # 除外リストまたはプライベート属性はスキップ\n",
        "    if attr_name in nn_except or attr_name.startswith(\"__\"):\n",
        "        continue\n",
        "\n",
        "    # ★重要: モジュール型の属性は削除しない（attention, functional等）\n",
        "    if inspect.ismodule(attr_value):\n",
        "        continue\n",
        "\n",
        "    # それ以外のクラス・関数(nn.Linear, nn.ReLUなど)を削除\n",
        "    try:\n",
        "        delattr(nn, attr_name)\n",
        "    except (AttributeError, TypeError):\n",
        "        pass\n",
        "\n",
        "print(f\"--- API制限適用完了 (許可: {nn_except} + サブモジュール) ---\")\n",
        "# ★★★ 改善完了 ★★★\n",
        "\n",
        "\n",
        "# --- シード固定関数 ---\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) # アンサンブルのためにallも設定\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- デバイス設定 ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 1. データ準備 (★改善点 2: Cutoutのバグ修正)\n",
        "# ===================================================================\n",
        "\n",
        "def augment_data_torch_cutout(images_flat, h_flip_prob=0.5, shift_pixels=2, cutout_size=8):\n",
        "    \"\"\"\n",
        "    torchvisionに依存しない、テンソル演算のみによるデータ拡張。\n",
        "    \"\"\"\n",
        "    images_2d = images_flat.reshape(-1, 1, 28, 28)\n",
        "    n_images, c, h, w = images_2d.shape\n",
        "\n",
        "    # --- 1. 水平フリップ (p=0.5) ---\n",
        "    flip_mask = torch.rand(n_images, device=images_flat.device) < h_flip_prob\n",
        "    if flip_mask.any():\n",
        "        images_2d[flip_mask] = torch.flip(images_2d[flip_mask], dims=[3])\n",
        "\n",
        "    # --- 2. ランダムシフト (±2ピクセル) ---\n",
        "    shifts = torch.randint(-shift_pixels, shift_pixels + 1, (n_images, 2), device=images_flat.device)\n",
        "    # nn.functional.pad は安全なAPI制限により削除されていないため使用可能\n",
        "    padded_images = torch.nn.functional.pad(images_2d, (shift_pixels, shift_pixels, shift_pixels, shift_pixels), mode='constant', value=0.0)\n",
        "    augmented_shifted = torch.empty_like(images_2d)\n",
        "\n",
        "    for i in range(n_images):\n",
        "        shift_y, shift_x = shifts[i, 0].item(), shifts[i, 1].item()\n",
        "        y_start = shift_pixels + shift_y\n",
        "        x_start = shift_pixels + shift_x\n",
        "        augmented_shifted[i] = padded_images[i, :, y_start:y_start+h, x_start:x_start+w]\n",
        "\n",
        "    images_2d = augmented_shifted\n",
        "\n",
        "    # --- 3. カットアウト (Cutout) ---\n",
        "    cutout_half = cutout_size // 2\n",
        "    center_y = torch.randint(0, h, (n_images,), device=images_flat.device)\n",
        "    center_x = torch.randint(0, w, (n_images,), device=images_flat.device)\n",
        "\n",
        "    for i in range(n_images):\n",
        "        y1 = torch.clamp(center_y[i] - cutout_half, 0, h).item()\n",
        "        y2 = torch.clamp(center_y[i] + cutout_half, 0, h).item()\n",
        "\n",
        "        # ★★★ 改善点 2: バグ修正 ★★★\n",
        "        # x1 と x2 の計算が両方 '+' になっていたのを修正\n",
        "        # x1 = torch.clamp(center_x[i] + cutout_half, 0, w).item() # <- バグ\n",
        "        # x2 = torch.clamp(center_x[i] + cutout_half, 0, w).item() # <- バグ\n",
        "        x1 = torch.clamp(center_x[i] - cutout_half, 0, w).item() # <- 修正\n",
        "        x2 = torch.clamp(center_x[i] + cutout_half, 0, w).item() # <- 修正\n",
        "\n",
        "        # 範囲チェック (y1 < y2 and x1 < x2)\n",
        "        if y1 < y2 and x1 < x2:\n",
        "            images_2d[i, :, y1:y2, x1:x2] = 0.0\n",
        "    # ★★★ 改善完了 ★★★\n",
        "\n",
        "    return images_2d.reshape(-1, 784)\n",
        "\n",
        "# --- データセットクラス (torchvision 依存の排除) ---\n",
        "# 元の .ipynb にあったシンプルなDatasetクラス\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, x_data, t_label):\n",
        "        # ★改善: 0-1に正規化 (augment_data_torch_cutoutは0-1を前提)\n",
        "        self.x_data = x_data.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "        self.t_label = t_label\n",
        "        self.t_hot = np.eye(10)[t_label].astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # テンソルに変換して返す\n",
        "        img_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.t_label[idx], dtype=torch.long)\n",
        "        hot_label = torch.tensor(self.t_hot[idx], dtype=torch.float32)\n",
        "\n",
        "        # (Tensor[784], Tensor[10], Tensor[long]) を返す\n",
        "        return img_tensor, hot_label, label\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, x_data):\n",
        "        self.x_data = x_data.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
        "        return img_tensor # テスト時は画像データのみ返す\n",
        "\n",
        "# --- データのロードと分割 ---\n",
        "data_path = os.path.join(work_dir, 'Lecture04', 'data')\n",
        "x_train_all = np.load(os.path.join(data_path, 'x_train.npy'))\n",
        "t_train_all = np.load(os.path.join(data_path, 'y_train.npy'))\n",
        "x_test = np.load(os.path.join(data_path, 'x_test.npy'))\n",
        "\n",
        "x_train_split, x_valid_split, t_train_split_label, t_valid_split_label = train_test_split(\n",
        "    x_train_all, t_train_all, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"訓練データ: {x_train_split.shape}, 検証データ: {x_valid_split.shape}\")\n",
        "\n",
        "# --- データローダーの作成 ---\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "\n",
        "train_data = FashionDataset(x_train_split, t_train_split_label)\n",
        "valid_data = FashionDataset(x_valid_split, t_valid_split_label)\n",
        "test_data = TestDataset(x_test)\n",
        "\n",
        "dataloader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "dataloader_valid = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "dataloader_test = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"--- データローダー準備完了 (torchvision非依存) ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. PyTorchによるニューラルネットワークのクラス化 (ルール準拠)\n",
        "# ===================================================================\n",
        "\n",
        "def relu(x):\n",
        "    return torch.where(x > 0, x, torch.zeros_like(x))\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, is_relu=True):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty(in_dim, out_dim, dtype=torch.float32))\n",
        "        self.b = nn.Parameter(torch.empty(out_dim, dtype=torch.float32))\n",
        "        self.initialize_parameters(is_relu)\n",
        "\n",
        "    def initialize_parameters(self, is_relu):\n",
        "        if is_relu:\n",
        "            nn.init.kaiming_normal_(self.W, mode='fan_in', nonlinearity='relu')\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.W)\n",
        "        nn.init.zeros_(self.b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.W) + self.b\n",
        "\n",
        "class BatchNorm1d(nn.Module):\n",
        "    def __init__(self, dim, momentum=0.9, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim).float())\n",
        "        self.beta = nn.Parameter(torch.zeros(dim).float())\n",
        "        self.register_buffer('running_mean', torch.zeros(dim).float())\n",
        "        self.register_buffer('running_var', torch.ones(dim).float())\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            if x.shape[0] > 1:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = x.var(axis=0, unbiased=False)\n",
        "            else:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = torch.zeros_like(mu)\n",
        "            self.running_mean = self.momentum * self.running_mean.detach() + (1.0 - self.momentum) * mu.detach()\n",
        "            self.running_var = self.momentum * self.running_var.detach() + (1.0 - self.momentum) * var.detach()\n",
        "            x_norm = (x - mu) / torch.sqrt(var + self.eps)\n",
        "        else:\n",
        "            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "class Dropout(nn.Module):\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.dropout_ratio > 0.0:\n",
        "            drop_mask = (torch.rand_like(x) > self.dropout_ratio).float() / (1.0 - self.dropout_ratio)\n",
        "            return x * drop_mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum):\n",
        "        super().__init__()\n",
        "        self.fc1 = Dense(in_dim, hidden_sizes[0], is_relu=True)\n",
        "        self.bn1 = BatchNorm1d(hidden_sizes[0], momentum=bn_momentum)\n",
        "        self.drop1 = Dropout(dropout_ratio)\n",
        "        self.fc2 = Dense(hidden_sizes[0], hidden_sizes[1], is_relu=True)\n",
        "        self.bn2 = BatchNorm1d(hidden_sizes[1], momentum=bn_momentum)\n",
        "        self.drop2 = Dropout(dropout_ratio)\n",
        "        self.fc3 = Dense(hidden_sizes[1], hidden_sizes[2], is_relu=True)\n",
        "        self.bn3 = BatchNorm1d(hidden_sizes[2], momentum=bn_momentum)\n",
        "        self.drop3 = Dropout(dropout_ratio)\n",
        "        self.output_layer = Dense(hidden_sizes[2], out_dim, is_relu=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力は (B, 784)\n",
        "        x = relu(self.drop1(self.bn1(self.fc1(x))))\n",
        "        x = relu(self.drop2(self.bn2(self.fc2(x))))\n",
        "        x = relu(self.drop3(self.bn3(self.fc3(x))))\n",
        "        x = self.output_layer(x)\n",
        "        return x # ロジットを返す\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. アンサンブル学習ループ (★改善点 2, 3, 4)\n",
        "# ===================================================================\n",
        "\n",
        "# --- ハイパーパラメータ設定 ---\n",
        "in_dim = 784\n",
        "out_dim = 10\n",
        "hidden_sizes = [1024, 512, 256]\n",
        "bn_momentum = 0.9\n",
        "lr = 0.001\n",
        "reg_lambda = 1e-5\n",
        "\n",
        "# ★アイデア 2: 過学習抑制のためDropoutを強化\n",
        "dropout_ratio = 0.4 # 0.3 -> 0.4 (0.5なども試す価値あり)\n",
        "\n",
        "# ★要望: Epoch 500\n",
        "n_epochs = 500\n",
        "patience = 60 # 60エポック (約600イテレーション) 改善がなければ停止\n",
        "\n",
        "# ★要望: アンサンブル\n",
        "ensemble_seeds = [42, 123, 555] # 3つの異なるシードでモデルを訓練\n",
        "all_best_model_states = []\n",
        "all_best_valid_accs = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"--- アンサンブル学習開始 (Models: {len(ensemble_seeds)}, Epochs: {n_epochs}, Dropout: {dropout_ratio}) ---\")\n",
        "print(\"--- ★データ拡張: 手動Cutout + Shift + Flip (torchvision非依存) ---\")\n",
        "\n",
        "for i, seed in enumerate(ensemble_seeds):\n",
        "    print(f\"\\n--- モデル {i+1}/{len(ensemble_seeds)} (Seed: {seed}) の学習開始 ---\")\n",
        "\n",
        "    # ループごとにシードを固定\n",
        "    set_seed(seed)\n",
        "\n",
        "    # モデル、オプティマイザ、スケジューラを初期化\n",
        "    mlp = MLP(in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum).to(device)\n",
        "    optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=reg_lambda)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
        "\n",
        "    best_valid_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        losses_train = []\n",
        "        train_num = 0\n",
        "        train_true_num = 0\n",
        "\n",
        "        mlp.train()  # 訓練時には勾配を計算するtrainモードにする\n",
        "        # dataloader_train は (img_tensor[784], t_hot[10], t_label[long]) を返す\n",
        "        for x, t_hot, t_label in dataloader_train:\n",
        "            x, t_label = x.to(device), t_label.to(device)\n",
        "\n",
        "            # --- ★手動データ拡張 (Cutout + Shift + Flip) ---\n",
        "            x_aug = augment_data_torch_cutout(x,\n",
        "                                                h_flip_prob=0.5,\n",
        "                                                shift_pixels=2,\n",
        "                                                cutout_size=8)\n",
        "\n",
        "            y_logits = mlp(x_aug) # 拡張後のデータで学習\n",
        "            loss = criterion(y_logits, t_label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses_train.append(loss.tolist())\n",
        "            pred = y_logits.argmax(1)\n",
        "            acc = (pred == t_label).float()\n",
        "            train_num += acc.size(0)\n",
        "            train_true_num += acc.sum().item()\n",
        "\n",
        "        # --- 検証 (★手動TTA: 水平フリップ) ---\n",
        "        mlp.eval()  # 評価時には勾配を計算しないevalモードにする\n",
        "        losses_valid = []\n",
        "        valid_num = 0\n",
        "        valid_true_num = 0\n",
        "        with torch.no_grad():\n",
        "            for x, t_hot, t_label in dataloader_valid:\n",
        "                x, t_label = x.to(device), t_label.to(device)\n",
        "\n",
        "                # TTA (1): 元の画像 (B, 784)\n",
        "                y_logits_original = mlp(x)\n",
        "\n",
        "                # TTA (2): 水平反転した画像\n",
        "                x_reshaped = x.reshape(-1, 1, 28, 28)\n",
        "                x_flipped = torch.flip(x_reshaped, dims=[3]) # (B, 1, 28, 28)\n",
        "                x_flipped_flat = x_flipped.reshape(-1, 784) # (B, 784)\n",
        "                y_logits_flipped = mlp(x_flipped_flat)\n",
        "\n",
        "                y_logits_avg = (y_logits_original + y_logits_flipped) / 2.0\n",
        "\n",
        "                loss = criterion(y_logits_avg, t_label)\n",
        "                losses_valid.append(loss.tolist())\n",
        "\n",
        "                pred = y_logits_avg.argmax(1)\n",
        "                acc = (pred == t_label).float()\n",
        "                valid_num += acc.size(0)\n",
        "                valid_true_num += acc.sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        current_train_accuracy = train_true_num / train_num\n",
        "        current_valid_accuracy = valid_true_num / valid_num\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f'EPOCH: {epoch+1:3d}, Train [Loss: {np.mean(losses_train):.4f}, Acc: {current_train_accuracy:.4f}], '\n",
        "                  f'Valid [Loss: {np.mean(losses_valid):.4f}, Acc(TTA): {current_valid_accuracy:.4f}], '\n",
        "                  f'LR: {current_lr:.6f}')\n",
        "\n",
        "        if current_valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = current_valid_accuracy\n",
        "            patience_counter = 0\n",
        "            best_model_state = copy.deepcopy(mlp.state_dict())\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f\"    -> Best TTA Valid Accuracy Updated: {best_valid_accuracy:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n--- Early stopping triggered after {epoch+1} epochs. ---\")\n",
        "            break\n",
        "\n",
        "    # このシードでのベストモデルと精度を保存\n",
        "    all_best_model_states.append(best_model_state)\n",
        "    all_best_valid_accs.append(best_valid_accuracy)\n",
        "    print(f\"--- モデル {i+1} 完了 (Best Valid Acc: {best_valid_accuracy:.4f}) ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 最終予測 (アンサンブルと手動TTA)\n",
        "# ===================================================================\n",
        "print(\"\\n--- 全モデルの学習完了 ---\")\n",
        "print(f\"各モデルのベスト検証精度 (TTA): {[round(acc, 4) for acc in all_best_valid_accs]}\")\n",
        "print(f\"平均検証精度 (TTA): {np.mean(all_best_valid_accs):.4f}\")\n",
        "print(\"\\n--- アンサンブルとTTAによる最終予測の生成 ---\")\n",
        "\n",
        "all_test_logits = [] # [モデル数] x [データ数, クラス数] のリスト\n",
        "\n",
        "# MLPモデルのインスタンスを再作成 (重みをロードするため)\n",
        "mlp = MLP(in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum).to(device)\n",
        "mlp.eval() # 常に評価モード\n",
        "\n",
        "for model_state in all_best_model_states:\n",
        "    if model_state is None:\n",
        "        print(\"警告: 訓練されなかったモデル（stateがNone）をスキップします。\")\n",
        "        continue\n",
        "\n",
        "    mlp.load_state_dict(model_state)\n",
        "\n",
        "    model_test_logits = [] # このモデルの予測（ロジット）\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in dataloader_test:\n",
        "            x = x.to(device)\n",
        "\n",
        "            # TTA (1): 元の画像\n",
        "            y_logits_original = mlp.forward(x)\n",
        "\n",
        "            # TTA (2): 水平反転した画像\n",
        "            x_reshaped = x.reshape(-1, 1, 28, 28)\n",
        "            x_flipped = torch.flip(x_reshaped, dims=[3])\n",
        "            x_flipped_flat = x_flipped.reshape(-1, 784)\n",
        "            y_logits_flipped = mlp.forward(x_flipped_flat)\n",
        "\n",
        "            y_logits_avg = (y_logits_original + y_logits_flipped) / 2.0\n",
        "\n",
        "            model_test_logits.append(y_logits_avg.cpu()) # CPUに集める\n",
        "\n",
        "    # このモデルの全テストデータに対する予測を結合\n",
        "    all_test_logits.append(torch.cat(model_test_logits, dim=0))\n",
        "\n",
        "if not all_test_logits:\n",
        "    print(\"エラー: 有効なモデルが1つも訓練されませんでした。\")\n",
        "else:\n",
        "    # (モデル数, データ数, クラス数) のテンソルにスタック\n",
        "    stacked_logits = torch.stack(all_test_logits, dim=0)\n",
        "\n",
        "    # (モデル数) の軸で平均を取る (アンサンブル)\n",
        "    mean_logits = torch.mean(stacked_logits, dim=0) # (データ数, クラス数)\n",
        "\n",
        "    # 最終予測\n",
        "    final_pred = mean_logits.argmax(1).tolist()\n",
        "\n",
        "    # --- 提出ファイルの保存 ---\n",
        "    submission = pd.Series(final_pred, name='label')\n",
        "    submission_filename = os.path.join(work_dir, 'Lecture04', 'submission_pred_06_ensemble_tta.csv')\n",
        "    os.makedirs(os.path.dirname(submission_filename), exist_ok=True)\n",
        "    submission.to_csv(submission_filename, header=True, index_label='id')\n",
        "    print(f\"\\n--- 予測完了。提出ファイル '{submission_filename}' を保存しました。 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "5aNTxlv9pZ6_",
        "outputId": "c889fdca-e2d5-45b8-9bb4-ae830f91953c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "作業ディレクトリ: /content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab\n",
            "--- API制限適用完了 (許可: ['Module', 'Parameter', 'Sequential', 'parameter', 'init', 'CrossEntropyLoss', 'ModuleList'] + サブモジュール) ---\n",
            "使用デバイス: cuda\n",
            "訓練データ: (48000, 28, 28), 検証データ: (12000, 28, 28)\n",
            "--- データローダー準備完了 (torchvision非依存) ---\n",
            "--- アンサンブル学習開始 (Models: 3, Epochs: 500, Dropout: 0.4) ---\n",
            "--- ★データ拡張: 手動Cutout + Shift + Flip (torchvision非依存) ---\n",
            "\n",
            "--- モデル 1/3 (Seed: 42) の学習開始 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'ModuleDict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2918182771.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# モデル、オプティマイザ、スケジューラを初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_momentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdecoupled_weight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoupled_weight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# We can safely turn off functools.wraps here because the inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresume_execution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTORCH_DYNAMO_RESUME_IN_PREFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP_SUPPORTED_MODULES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrap_if_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m from .variables import (\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mFunctionalCallVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnumVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprocess_automatic_dynamic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mside_effects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSideEffects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m from ..source import (\n\u001b[1;32m     95\u001b[0m     \u001b[0mAttrProxySource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/side_effects.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcreate_instruction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyCodegen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSideEffectsError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munimplemented_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocalCellSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocalSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/codegen.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_safe_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_n_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueMutationExisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m from .variables.functions import (\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mContextlibContextManagerLocalGeneratorObjectVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mLocalGeneratorObjectVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fully_shard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fsdp_param_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0m_fsdp_param_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mUnshardHandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m from .fully_sharded_data_parallel import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBackwardPrefetch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mCPUOffload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_annotate_modules_for_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from torch.distributed.fsdp._init_utils import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0m_check_orig_params_flattened\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0m_init_buffer_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_init_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mStateDictType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTensorExtensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_sync_params_and_buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/wrap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;31m# Set those defaults to the size_based_auto_wrap_policy function. Make them easy to be imported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m \u001b[0msize_based_auto_wrap_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCLUDE_WRAP_MODULES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDict\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0msize_based_auto_wrap_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFORCE_LEAF_MODULES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'ModuleDict'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "アンサンブルの改善"
      ],
      "metadata": {
        "id": "D4Wht3jzpNk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# API制限のコードと，torchに統合されているdynamoライブラリの挙動がコンフリクトを起こすため，dynamoを無効化\n",
        "import torch._dynamo\n",
        "torch._dynamo.disable()\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import inspect\n",
        "\n",
        "\n",
        "# === API制限 (ユーザー提供コード) ===\n",
        "# ModuleListと parameter を追加\n",
        "nn_except = [\"Module\", \"Parameter\", \"Sequential\", \"ModuleList\", \"parameter\"]\n",
        "for m in inspect.getmembers(nn):\n",
        "    if not m[0] in nn_except and m[0][0:2] != \"__\":\n",
        "        delattr(nn, m[0])\n",
        "# --- API制限ここまで ---\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 0. グローバル設定 (シード固定, デバイス)\n",
        "# ===================================================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"シードを固定して再現性を確保する\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# データ分割用のシード (Numpy版と合わせる)\n",
        "BASE_SEED = 42\n",
        "set_seed(BASE_SEED) # まずはベースシードで固定\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===================================================================\n",
        "# 1. データ読み込みと前処理\n",
        "# ===================================================================\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) # 再マウントを強制\n",
        "    # (!!! この部分はご自身の環境に合わせて必ず設定してください !!!)\n",
        "    work_dir = '/content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab' # ← ★★★ 要変更 ★★★\n",
        "\n",
        "    if not os.path.exists(work_dir):\n",
        "        print(f\"警告: ディレクトリ '{work_dir}' が存在しません。\")\n",
        "        work_dir = '.'\n",
        "        data_dir = work_dir + '/Lecture04/data'\n",
        "    else:\n",
        "        data_dir = work_dir + '/Lecture04/data'\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Google Colab 環境ではないため、ローカルカレントディレクトリを参照します。\")\n",
        "    work_dir = '.'\n",
        "    data_dir = work_dir + '/Lecture04/data' # 宿題のパスを想定\n",
        "\n",
        "print(f\"データディレクトリ: {data_dir}\")\n",
        "print(f\"デバイス: {device}\")\n",
        "\n",
        "# 学習データ\n",
        "try:\n",
        "    x_train = np.load(data_dir + '/x_train.npy')\n",
        "    t_train = np.load(data_dir + '/y_train.npy')\n",
        "    # テストデータ\n",
        "    x_test = np.load(data_dir + '/x_test.npy')\n",
        "    print(\"Numpyデータを正常にロードしました。\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"エラー: {data_dir} でデータファイルが見つかりません。\")\n",
        "    print(\"代替策: カレントディレクトリから .npy ファイルを読み込みます...\")\n",
        "    try:\n",
        "        x_train = np.load('x_train.npy')\n",
        "        t_train = np.load('y_train.npy')\n",
        "        x_test = np.load('x_test.npy')\n",
        "        work_dir = '.' # 保存先をカレントに\n",
        "        data_dir = '.'\n",
        "        print(\"カレントディレクトリからデータを読み込みました。\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"カレントディレクトリにもファイルがありません。処理を中断します。\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "# --- データの前処理 (Numpy) ---\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).astype('float32')\n",
        "t_train_onehot = np.eye(N=10)[t_train.astype(\"int32\").flatten()].astype('float32')\n",
        "\n",
        "# 検証データの分割 (BASE_SEED=42 で固定)\n",
        "x_train_split, x_valid_split, t_train_split_onehot, t_valid_split_onehot = train_test_split(\n",
        "    x_train, t_train_onehot, test_size=0.2, random_state=BASE_SEED\n",
        ")\n",
        "\n",
        "# --- PyTorch Dataset/DataLoader (ユーザー定義ベース) ---\n",
        "class FashionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_data, t_data_onehot):\n",
        "        self.x_data = torch.tensor(x_data, dtype=torch.float)\n",
        "        self.t_data_onehot = torch.tensor(t_data_onehot, dtype=torch.float)\n",
        "        self.t_data_label = torch.tensor(np.argmax(t_data_onehot, axis=1), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_data[idx], self.t_data_onehot[idx], self.t_data_label[idx]\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_test):\n",
        "        self.x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_test.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_test[idx]\n",
        "\n",
        "# DataLoader の設定\n",
        "batch_size = 128\n",
        "train_data = FashionDataset(x_train_split, t_train_split_onehot)\n",
        "valid_data = FashionDataset(x_valid_split, t_valid_split_onehot)\n",
        "test_data = TestDataset(x_test)\n",
        "\n",
        "# (DataLoaderは学習ループ内でシード毎に再生成)\n",
        "print(\"\\n--- データセット準備完了 ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. 改善策 (安定化、拡張、TTA)\n",
        "# ===================================================================\n",
        "\n",
        "# --- 1. 補助関数 (ReLU) ---\n",
        "def relu(x):\n",
        "    # ユーザーコードの relu (torch.where) は低速な可能性があるため、clampに変更\n",
        "    return torch.clamp(x, min=0.0)\n",
        "    # return torch.where(x > 0, x, torch.zeros_like(x)) # ユーザー版\n",
        "\n",
        "# --- 改善策 1: 数値的に安定な損失関数 (LogSumExp) ---\n",
        "def stable_cross_entropy_loss(logits, t_hot):\n",
        "    \"\"\"\n",
        "    ロジット (logits) を入力とし、数値的に安定なクロスエントロピー損失を計算\n",
        "    \"\"\"\n",
        "    # LogSumExpトリック: log(softmax(x)) = x - log(sum(exp(x)))\n",
        "    log_softmax_manual = logits - torch.logsumexp(logits, dim=1, keepdim=True)\n",
        "    # NLLLoss (Negative Log Likelihood Loss)\n",
        "    return -(t_hot * log_softmax_manual).sum(dim=1).mean()\n",
        "\n",
        "# --- 改善策 2: データ拡張 (Shift, Flip + Cutout) ---\n",
        "def augment_data_torch_cutout(images_784, h_flip_prob=0.1, shift_pixels=1, cutout_size=8):\n",
        "    \"\"\"\n",
        "    (N, 784) のテンソルを入力とし、Shift, Flip, Cutout を適用\n",
        "    \"\"\"\n",
        "    images_2d = images_784.reshape(-1, 28, 28)\n",
        "    n_images, h, w = images_2d.shape\n",
        "    device = images_784.device\n",
        "\n",
        "    # --- 1. Shift (±1ピクセル) ---\n",
        "    shifts = torch.randint(-shift_pixels, shift_pixels + 1, (n_images, 2), device=device)\n",
        "    augmented_images = torch.zeros_like(images_2d) # Shift適用後のテンポラリ\n",
        "\n",
        "    for i in range(n_images):\n",
        "        img = images_2d[i]\n",
        "        shift_y, shift_x = shifts[i, 0].item(), shifts[i, 1].item()\n",
        "        y_start_dest, y_end_dest = max(0, shift_y), min(h, h + shift_y)\n",
        "        x_start_dest, x_end_dest = max(0, shift_x), min(w, w + shift_x)\n",
        "        y_start_src, y_end_src = max(0, -shift_y), min(h, h - shift_y)\n",
        "        x_start_src, x_end_src = max(0, -shift_x), min(w, w - shift_x)\n",
        "        if x_start_dest < x_end_dest and y_start_dest < y_end_dest:\n",
        "            augmented_images[i, y_start_dest:y_end_dest, x_start_dest:x_end_dest] = \\\n",
        "                img[y_start_src:y_end_src, x_start_src:x_end_src]\n",
        "\n",
        "    # --- 2. Horizontal Flip (10%の確率) ---\n",
        "    flip_mask = torch.rand(n_images, device=device) < h_flip_prob\n",
        "    if flip_mask.any():\n",
        "        augmented_images[flip_mask] = torch.flip(augmented_images[flip_mask], dims=[2])\n",
        "\n",
        "    # --- 3. Cutout (★ 改善点) ---\n",
        "    if cutout_size > 0:\n",
        "        # (N) 個のランダムな座標を生成 (バッチ処理)\n",
        "        y_starts = torch.randint(0, h - cutout_size + 1, (n_images,), device=device)\n",
        "        x_starts = torch.randint(0, w - cutout_size + 1, (n_images,), device=device)\n",
        "        for i in range(n_images):\n",
        "            # 矩形領域を 0 でマスク\n",
        "            y1, y2 = y_starts[i], y_starts[i] + cutout_size\n",
        "            x1, x2 = x_starts[i], x_starts[i] + cutout_size\n",
        "            augmented_images[i, y1:y2, x1:x2] = 0.0\n",
        "\n",
        "    return augmented_images.reshape(-1, 784)\n",
        "\n",
        "\n",
        "# --- 改善策 3: TTA (Test-Time Augmentation) ---\n",
        "def predict_with_tta(model, x_784):\n",
        "    \"\"\"\n",
        "    TTAを適用し、平均ロジット（logits）を返す\n",
        "    \"\"\"\n",
        "    model.eval() # 念のため評価モード\n",
        "    # 1. 元画像のロジット\n",
        "    logits_original = model(x_784)\n",
        "\n",
        "    # 2. 反転画像のロジット\n",
        "    x_2d = x_784.reshape(-1, 28, 28)\n",
        "    x_2d_flipped = torch.flip(x_2d, dims=[2]) # W次元(dim=2)で反転\n",
        "    x_784_flipped = x_2d_flipped.reshape(-1, 784)\n",
        "    logits_flipped = model(x_784_flipped)\n",
        "\n",
        "    # 3. 平均ロジット\n",
        "    return (logits_original + logits_flipped) / 2.0\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. カスタムモジュール (ユーザーコードベース)\n",
        "# ===================================================================\n",
        "\n",
        "# 3.1. Dense (Linear) レイヤ\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, is_relu=True):\n",
        "        super().__init__()\n",
        "        # ユーザーコードの一様分布初期化を維持\n",
        "        scale = math.sqrt(2.0 / in_dim) if is_relu else math.sqrt(1.0 / in_dim)\n",
        "        W_data = (torch.rand(in_dim, out_dim) * 2 - 1) * scale\n",
        "        self.W = nn.Parameter(W_data.float())\n",
        "        b_data = torch.zeros(out_dim)\n",
        "        self.b = nn.Parameter(b_data.float())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.W) + self.b\n",
        "\n",
        "# 3.2. BatchNorm1d レイヤ (ユーザーコードのロジックを維持)\n",
        "class BatchNorm1d(nn.Module):\n",
        "    def __init__(self, dim, momentum=0.9, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim).float())\n",
        "        self.beta = nn.Parameter(torch.zeros(dim).float())\n",
        "        self.register_buffer('running_mean', torch.zeros(dim).float())\n",
        "        self.register_buffer('running_var', torch.ones(dim).float()) # 1で初期化\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            if x.shape[0] > 1:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = x.var(axis=0, unbiased=False)\n",
        "            else:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = torch.zeros_like(mu)\n",
        "\n",
        "            # Numpy版の momentum (0.9) をそのまま使用するロジック (ユーザーコード準拠)\n",
        "            self.running_mean.copy_(self.momentum * self.running_mean.detach() + (1.0 - self.momentum) * mu.detach())\n",
        "            self.running_var.copy_(self.momentum * self.running_var.detach() + (1.0 - self.momentum) * var.detach())\n",
        "            x_norm = (x - mu) / torch.sqrt(var + self.eps)\n",
        "        else:\n",
        "            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "        return self.gamma * x_norm\n",
        "\n",
        "# 3.3. Dropout レイヤ\n",
        "class Dropout(nn.Module):\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.scale = 1.0 / (1.0 - self.dropout_ratio) if self.dropout_ratio < 1.0 else 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.dropout_ratio > 0.0:\n",
        "            drop_mask = (torch.rand_like(x) > self.dropout_ratio).float() * self.scale\n",
        "            return x * drop_mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# 3.4. MLPモデル (★ 改善: ロジットを返すように変更)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum):\n",
        "        super().__init__()\n",
        "\n",
        "        # ユーザーコードの Sequential + 手動ReLU の構成を維持\n",
        "        self.hidden_layers = nn.Sequential(\n",
        "            Dense(in_dim, hidden_sizes[0], is_relu=True),\n",
        "            BatchNorm1d(hidden_sizes[0], momentum=bn_momentum),\n",
        "            Dropout(dropout_ratio),\n",
        "\n",
        "            Dense(hidden_sizes[0], hidden_sizes[1], is_relu=True),\n",
        "            BatchNorm1d(hidden_sizes[1], momentum=bn_momentum),\n",
        "            Dropout(dropout_ratio),\n",
        "\n",
        "            Dense(hidden_sizes[1], hidden_sizes[2], is_relu=True),\n",
        "            BatchNorm1d(hidden_sizes[2], momentum=bn_momentum),\n",
        "            Dropout(dropout_ratio)\n",
        "        )\n",
        "        self.output_layer = Dense(hidden_sizes[2], out_dim, is_relu=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ユーザーコードのフォワードパスを維持\n",
        "        x = self.hidden_layers[0](x) # Dense 1\n",
        "        x = self.hidden_layers[1](x) # BN 1\n",
        "        x = self.hidden_layers[2](x) # Dropout 1\n",
        "        x = relu(x) # ReLU 1\n",
        "\n",
        "        x = self.hidden_layers[3](x) # Dense 2\n",
        "        x = self.hidden_layers[4](x) # BN 2\n",
        "        x = self.hidden_layers[5](x) # Dropout 2\n",
        "        x = relu(x) # ReLU 2\n",
        "\n",
        "        x = self.hidden_layers[6](x) # Dense 3\n",
        "        x = self.hidden_layers[7](x) # BN 3\n",
        "        x = self.hidden_layers[8](x) # Dropout 3\n",
        "        x = relu(x) # ReLU 3\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        # ★ 改善: softmax を削除し、ロジットをそのまま返す\n",
        "        # y = softmax(x)\n",
        "        # return y\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 学習・評価ループ (改善策適用)\n",
        "# ===================================================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, config):\n",
        "    model.train() # 訓練モード\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # (x, t_hot, t_label) を受け取る\n",
        "    for x, t_hot, t_label in loader:\n",
        "\n",
        "        # --- 改善策 2: Cutout適用 ---\n",
        "        x_aug = augment_data_torch_cutout(\n",
        "            x,\n",
        "            h_flip_prob=config['h_flip_prob'],\n",
        "            shift_pixels=config['shift_pixels'],\n",
        "            cutout_size=config['cutout_size']\n",
        "        )\n",
        "        x_aug, t_hot, t_label = x_aug.to(device), t_hot.to(device), t_label.to(device)\n",
        "\n",
        "        # 順伝播 (ロジットを取得)\n",
        "        y_logits = model(x_aug)\n",
        "\n",
        "        # --- 改善策 1: 安定な損失関数 ---\n",
        "        loss = stable_cross_entropy_loss(y_logits, t_hot)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = y_logits.argmax(1)\n",
        "        total_correct += (pred == t_label).sum().item()\n",
        "        total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def validate_one_epoch(model, loader):\n",
        "    model.eval() # 評価モード\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, t_hot, t_label in loader:\n",
        "            x, t_hot, t_label = x.to(device), t_hot.to(device), t_label.to(device)\n",
        "\n",
        "            # --- 改善策 3: TTA適用 ---\n",
        "            y_logits_avg = predict_with_tta(model, x)\n",
        "\n",
        "            # --- 改善策 1: 安定な損失関数 ---\n",
        "            loss = stable_cross_entropy_loss(y_logits_avg, t_hot)\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            pred = y_logits_avg.argmax(1)\n",
        "            total_correct += (pred == t_label).sum().item()\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. メイン実行ブロック (改善策 4: アンサンブル学習)\n",
        "# ===================================================================\n",
        "\n",
        "# --- 5.1. ハイパーパラメータ設定 ---\n",
        "in_dim = 784\n",
        "out_dim = 10\n",
        "hidden_sizes = [1024, 512, 256]\n",
        "bn_momentum = 0.9    # Numpy版のmomentum値 (ユーザーコード準拠)\n",
        "lr = 0.001           # 学習率 (Adam)\n",
        "reg_lambda = 1e-5    # L2正則化 (weight_decay)\n",
        "\n",
        "n_epochs = 500       # 総エポック数 (コサインアニーリングのため)\n",
        "patience = 60        # 早期終了のPatience (チェック6回分)\n",
        "\n",
        "# 改善策パラメータ\n",
        "config = {\n",
        "    'dropout_ratio': 0.15, # ユーザー設定値 (0.25~0.35で調整可)\n",
        "    'h_flip_prob': 0.5,   # ユーザー設定値\n",
        "    'shift_pixels': 1,\n",
        "    'cutout_size': 6      # ★ Cutout有効化\n",
        "}\n",
        "\n",
        "# --- 改善策 4: アンサンブル ---\n",
        "ensemble_seeds = [42, 123, 555] # 3つの異なるシード\n",
        "all_test_logits = [] # 各モデルのテストロジットを保存するリスト\n",
        "\n",
        "print(f\"\\n--- アンサンブル学習開始 (Models: {len(ensemble_seeds)}) ---\")\n",
        "print(f\"--- 改善策: StableLoss, Cutout(size={config['cutout_size']}), TTA ---\")\n",
        "\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# DataLoader (アンサンブルループの外で定義)\n",
        "# DataLoaderのワーカ間でシードが異なるようにする設定\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(BASE_SEED) # DataLoaderのshuffle用シード\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, shuffle=True,\n",
        "    worker_init_fn=seed_worker, generator=g\n",
        ")\n",
        "dataloader_valid = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
        "dataloader_test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "for i, seed in enumerate(ensemble_seeds):\n",
        "    print(f\"\\n--- モデル {i+1}/{len(ensemble_seeds)} (Seed: {seed}) の学習開始 ---\")\n",
        "    model_start_time = time.time()\n",
        "\n",
        "    # シード固定 (モデル初期化、Dropout, Augmentationのため)\n",
        "    set_seed(seed)\n",
        "\n",
        "    mlp = MLP(\n",
        "        in_dim,\n",
        "        hidden_sizes,\n",
        "        out_dim,\n",
        "        config['dropout_ratio'],\n",
        "        bn_momentum\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=reg_lambda)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
        "\n",
        "    best_valid_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train_one_epoch(mlp, dataloader_train, optimizer, config)\n",
        "        valid_loss, valid_acc = validate_one_epoch(mlp, dataloader_valid)\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f'EPOCH: {epoch+1:3d}, Train [Acc: {train_acc:.4f}], '\n",
        "                  f'Valid [Acc (TTA): {valid_acc:.4f}], '\n",
        "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        if valid_acc > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_acc\n",
        "            patience_counter = 0\n",
        "            best_model_state = copy.deepcopy(mlp.state_dict())\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                 print(f\"    -> Best Valid Accuracy Updated: {best_valid_accuracy:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n--- Early stopping triggered after {epoch+1} epochs. ---\")\n",
        "            break\n",
        "\n",
        "    model_end_time = time.time()\n",
        "    print(f\"--- モデル {i+1} 学習完了 (Time: {model_end_time - model_start_time:.1f} sec) ---\")\n",
        "    print(f\"    Best Valid Accuracy (TTA): {best_valid_accuracy:.4f}\")\n",
        "\n",
        "    # --- 5.2. テストデータでの予測 (TTA使用) ---\n",
        "    if best_model_state:\n",
        "        mlp.load_state_dict(best_model_state)\n",
        "\n",
        "    mlp.eval()\n",
        "    model_test_logits = []\n",
        "    with torch.no_grad():\n",
        "        for x_test_batch in dataloader_test:\n",
        "            x_test_batch = x_test_batch.to(device)\n",
        "            logits_avg = predict_with_tta(mlp, x_test_batch)\n",
        "            model_test_logits.append(logits_avg.cpu())\n",
        "\n",
        "    all_test_logits.append(torch.cat(model_test_logits, dim=0))\n",
        "\n",
        "\n",
        "# --- 5.3. アンサンブルによる最終予測 ---\n",
        "print(\"\\n--- 全モデルの学習完了 ---\")\n",
        "print(\"--- アンサンブルによる最終予測 (TTA + Ensemble) ---\")\n",
        "\n",
        "# (NumModels, N_Test, NumClasses) のテンソルにスタック\n",
        "stacked_logits = torch.stack(all_test_logits)\n",
        "\n",
        "# NumModels 次元で平均 (N_Test, NumClasses)\n",
        "avg_logits = torch.mean(stacked_logits, dim=0)\n",
        "\n",
        "# 最終予測\n",
        "predictions = torch.argmax(avg_logits, dim=1).numpy()\n",
        "submission = pd.Series(predictions, name='label')\n",
        "\n",
        "# 保存先パスの作成 (ユーザーコードのパスを尊重)\n",
        "submission_filename = os.path.join(work_dir, 'Lecture04', 'submission_pred_04_optimized_final.csv')\n",
        "os.makedirs(os.path.dirname(submission_filename), exist_ok=True)\n",
        "\n",
        "submission.to_csv(submission_filename, header=True, index_label='id')\n",
        "\n",
        "overall_end_time = time.time()\n",
        "print(f\"\\n--- 処理完了 (Total Time: {overall_end_time - overall_start_time:.1f} sec) ---\")\n",
        "print(f\"最適化された予測ファイルが保存されました: {submission_filename}\")\n",
        "print(\"目標スコア 0.95 の達成を期待します。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbI8kOLifWbs",
        "outputId": "2afce2e2-7c29-4af8-bca1-c963a37188fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "データディレクトリ: /content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab/Lecture04/data\n",
            "デバイス: cuda\n",
            "Numpyデータを正常にロードしました。\n",
            "\n",
            "--- データセット準備完了 ---\n",
            "\n",
            "--- アンサンブル学習開始 (Models: 3) ---\n",
            "--- 改善策: StableLoss, Cutout(size=6), TTA ---\n",
            "\n",
            "--- モデル 1/3 (Seed: 42) の学習開始 ---\n",
            "EPOCH:   1, Train [Acc: 0.7618], Valid [Acc (TTA): 0.8273], LR: 0.001000\n",
            "    -> Best Valid Accuracy Updated: 0.8273\n",
            "EPOCH:  10, Train [Acc: 0.8472], Valid [Acc (TTA): 0.8732], LR: 0.000999\n",
            "    -> Best Valid Accuracy Updated: 0.8732\n",
            "EPOCH:  20, Train [Acc: 0.8596], Valid [Acc (TTA): 0.8877], LR: 0.000996\n",
            "EPOCH:  30, Train [Acc: 0.8683], Valid [Acc (TTA): 0.8888], LR: 0.000991\n",
            "EPOCH:  40, Train [Acc: 0.8730], Valid [Acc (TTA): 0.8898], LR: 0.000984\n",
            "EPOCH:  50, Train [Acc: 0.8755], Valid [Acc (TTA): 0.8984], LR: 0.000976\n",
            "EPOCH:  60, Train [Acc: 0.8781], Valid [Acc (TTA): 0.8952], LR: 0.000965\n",
            "EPOCH:  70, Train [Acc: 0.8818], Valid [Acc (TTA): 0.8986], LR: 0.000952\n",
            "EPOCH:  80, Train [Acc: 0.8842], Valid [Acc (TTA): 0.9029], LR: 0.000938\n",
            "EPOCH:  90, Train [Acc: 0.8847], Valid [Acc (TTA): 0.9050], LR: 0.000922\n",
            "    -> Best Valid Accuracy Updated: 0.9050\n",
            "EPOCH: 100, Train [Acc: 0.8850], Valid [Acc (TTA): 0.9026], LR: 0.000905\n",
            "EPOCH: 110, Train [Acc: 0.8890], Valid [Acc (TTA): 0.9074], LR: 0.000885\n",
            "    -> Best Valid Accuracy Updated: 0.9074\n",
            "EPOCH: 120, Train [Acc: 0.8893], Valid [Acc (TTA): 0.9030], LR: 0.000864\n",
            "EPOCH: 130, Train [Acc: 0.8907], Valid [Acc (TTA): 0.9031], LR: 0.000842\n",
            "EPOCH: 140, Train [Acc: 0.8924], Valid [Acc (TTA): 0.9061], LR: 0.000819\n",
            "EPOCH: 150, Train [Acc: 0.8941], Valid [Acc (TTA): 0.9049], LR: 0.000794\n",
            "EPOCH: 160, Train [Acc: 0.8958], Valid [Acc (TTA): 0.9101], LR: 0.000768\n",
            "EPOCH: 170, Train [Acc: 0.8992], Valid [Acc (TTA): 0.9088], LR: 0.000741\n",
            "EPOCH: 180, Train [Acc: 0.8989], Valid [Acc (TTA): 0.9108], LR: 0.000713\n",
            "EPOCH: 190, Train [Acc: 0.9011], Valid [Acc (TTA): 0.9082], LR: 0.000684\n",
            "EPOCH: 200, Train [Acc: 0.9009], Valid [Acc (TTA): 0.9133], LR: 0.000655\n",
            "EPOCH: 210, Train [Acc: 0.9024], Valid [Acc (TTA): 0.9126], LR: 0.000624\n",
            "EPOCH: 220, Train [Acc: 0.9035], Valid [Acc (TTA): 0.9110], LR: 0.000594\n",
            "EPOCH: 230, Train [Acc: 0.9049], Valid [Acc (TTA): 0.9132], LR: 0.000563\n",
            "EPOCH: 240, Train [Acc: 0.9063], Valid [Acc (TTA): 0.9147], LR: 0.000531\n",
            "EPOCH: 250, Train [Acc: 0.9088], Valid [Acc (TTA): 0.9148], LR: 0.000500\n",
            "EPOCH: 260, Train [Acc: 0.9120], Valid [Acc (TTA): 0.9157], LR: 0.000469\n",
            "EPOCH: 270, Train [Acc: 0.9104], Valid [Acc (TTA): 0.9161], LR: 0.000437\n",
            "EPOCH: 280, Train [Acc: 0.9125], Valid [Acc (TTA): 0.9137], LR: 0.000406\n",
            "EPOCH: 290, Train [Acc: 0.9129], Valid [Acc (TTA): 0.9174], LR: 0.000376\n",
            "EPOCH: 300, Train [Acc: 0.9149], Valid [Acc (TTA): 0.9208], LR: 0.000345\n",
            "    -> Best Valid Accuracy Updated: 0.9208\n",
            "EPOCH: 310, Train [Acc: 0.9175], Valid [Acc (TTA): 0.9180], LR: 0.000316\n",
            "EPOCH: 320, Train [Acc: 0.9185], Valid [Acc (TTA): 0.9205], LR: 0.000287\n",
            "EPOCH: 330, Train [Acc: 0.9189], Valid [Acc (TTA): 0.9204], LR: 0.000259\n",
            "EPOCH: 340, Train [Acc: 0.9223], Valid [Acc (TTA): 0.9199], LR: 0.000232\n",
            "EPOCH: 350, Train [Acc: 0.9224], Valid [Acc (TTA): 0.9174], LR: 0.000206\n",
            "EPOCH: 360, Train [Acc: 0.9226], Valid [Acc (TTA): 0.9210], LR: 0.000181\n",
            "EPOCH: 370, Train [Acc: 0.9253], Valid [Acc (TTA): 0.9216], LR: 0.000158\n",
            "EPOCH: 380, Train [Acc: 0.9261], Valid [Acc (TTA): 0.9223], LR: 0.000136\n",
            "EPOCH: 390, Train [Acc: 0.9290], Valid [Acc (TTA): 0.9224], LR: 0.000115\n",
            "EPOCH: 400, Train [Acc: 0.9296], Valid [Acc (TTA): 0.9226], LR: 0.000095\n",
            "EPOCH: 410, Train [Acc: 0.9289], Valid [Acc (TTA): 0.9234], LR: 0.000078\n",
            "EPOCH: 420, Train [Acc: 0.9295], Valid [Acc (TTA): 0.9227], LR: 0.000062\n",
            "EPOCH: 430, Train [Acc: 0.9308], Valid [Acc (TTA): 0.9218], LR: 0.000048\n",
            "EPOCH: 440, Train [Acc: 0.9313], Valid [Acc (TTA): 0.9243], LR: 0.000035\n",
            "EPOCH: 450, Train [Acc: 0.9335], Valid [Acc (TTA): 0.9235], LR: 0.000024\n",
            "EPOCH: 460, Train [Acc: 0.9319], Valid [Acc (TTA): 0.9239], LR: 0.000016\n",
            "EPOCH: 470, Train [Acc: 0.9328], Valid [Acc (TTA): 0.9243], LR: 0.000009\n",
            "EPOCH: 480, Train [Acc: 0.9340], Valid [Acc (TTA): 0.9247], LR: 0.000004\n",
            "EPOCH: 490, Train [Acc: 0.9330], Valid [Acc (TTA): 0.9244], LR: 0.000001\n",
            "EPOCH: 500, Train [Acc: 0.9319], Valid [Acc (TTA): 0.9247], LR: 0.000000\n",
            "--- モデル 1 学習完了 (Time: 2725.4 sec) ---\n",
            "    Best Valid Accuracy (TTA): 0.9252\n",
            "\n",
            "--- モデル 2/3 (Seed: 123) の学習開始 ---\n",
            "EPOCH:   1, Train [Acc: 0.7565], Valid [Acc (TTA): 0.8211], LR: 0.001000\n",
            "    -> Best Valid Accuracy Updated: 0.8211\n",
            "EPOCH:  10, Train [Acc: 0.8431], Valid [Acc (TTA): 0.8701], LR: 0.000999\n",
            "EPOCH:  20, Train [Acc: 0.8591], Valid [Acc (TTA): 0.8812], LR: 0.000996\n",
            "EPOCH:  30, Train [Acc: 0.8667], Valid [Acc (TTA): 0.8888], LR: 0.000991\n",
            "EPOCH:  40, Train [Acc: 0.8736], Valid [Acc (TTA): 0.8920], LR: 0.000984\n",
            "EPOCH:  50, Train [Acc: 0.8748], Valid [Acc (TTA): 0.8988], LR: 0.000976\n",
            "    -> Best Valid Accuracy Updated: 0.8988\n",
            "EPOCH:  60, Train [Acc: 0.8777], Valid [Acc (TTA): 0.8948], LR: 0.000965\n",
            "EPOCH:  70, Train [Acc: 0.8828], Valid [Acc (TTA): 0.8992], LR: 0.000952\n",
            "EPOCH:  80, Train [Acc: 0.8827], Valid [Acc (TTA): 0.8996], LR: 0.000938\n",
            "EPOCH:  90, Train [Acc: 0.8849], Valid [Acc (TTA): 0.8989], LR: 0.000922\n",
            "EPOCH: 100, Train [Acc: 0.8864], Valid [Acc (TTA): 0.9066], LR: 0.000905\n",
            "    -> Best Valid Accuracy Updated: 0.9066\n",
            "EPOCH: 110, Train [Acc: 0.8894], Valid [Acc (TTA): 0.8926], LR: 0.000885\n",
            "EPOCH: 120, Train [Acc: 0.8911], Valid [Acc (TTA): 0.9073], LR: 0.000864\n",
            "EPOCH: 130, Train [Acc: 0.8931], Valid [Acc (TTA): 0.9077], LR: 0.000842\n",
            "EPOCH: 140, Train [Acc: 0.8931], Valid [Acc (TTA): 0.9049], LR: 0.000819\n",
            "EPOCH: 150, Train [Acc: 0.8925], Valid [Acc (TTA): 0.9067], LR: 0.000794\n",
            "EPOCH: 160, Train [Acc: 0.8959], Valid [Acc (TTA): 0.9088], LR: 0.000768\n",
            "EPOCH: 170, Train [Acc: 0.8977], Valid [Acc (TTA): 0.9106], LR: 0.000741\n",
            "EPOCH: 180, Train [Acc: 0.8990], Valid [Acc (TTA): 0.9118], LR: 0.000713\n",
            "EPOCH: 190, Train [Acc: 0.9009], Valid [Acc (TTA): 0.9111], LR: 0.000684\n",
            "EPOCH: 200, Train [Acc: 0.9016], Valid [Acc (TTA): 0.9116], LR: 0.000655\n",
            "EPOCH: 210, Train [Acc: 0.9017], Valid [Acc (TTA): 0.9113], LR: 0.000624\n",
            "EPOCH: 220, Train [Acc: 0.9027], Valid [Acc (TTA): 0.9126], LR: 0.000594\n",
            "EPOCH: 230, Train [Acc: 0.9053], Valid [Acc (TTA): 0.9143], LR: 0.000563\n",
            "EPOCH: 240, Train [Acc: 0.9062], Valid [Acc (TTA): 0.9127], LR: 0.000531\n",
            "EPOCH: 250, Train [Acc: 0.9088], Valid [Acc (TTA): 0.9169], LR: 0.000500\n",
            "    -> Best Valid Accuracy Updated: 0.9169\n",
            "EPOCH: 260, Train [Acc: 0.9089], Valid [Acc (TTA): 0.9121], LR: 0.000469\n",
            "EPOCH: 270, Train [Acc: 0.9103], Valid [Acc (TTA): 0.9181], LR: 0.000437\n",
            "    -> Best Valid Accuracy Updated: 0.9181\n",
            "EPOCH: 280, Train [Acc: 0.9124], Valid [Acc (TTA): 0.9172], LR: 0.000406\n",
            "EPOCH: 290, Train [Acc: 0.9140], Valid [Acc (TTA): 0.9136], LR: 0.000376\n",
            "EPOCH: 300, Train [Acc: 0.9140], Valid [Acc (TTA): 0.9166], LR: 0.000345\n",
            "EPOCH: 310, Train [Acc: 0.9155], Valid [Acc (TTA): 0.9177], LR: 0.000316\n",
            "EPOCH: 320, Train [Acc: 0.9189], Valid [Acc (TTA): 0.9164], LR: 0.000287\n",
            "EPOCH: 330, Train [Acc: 0.9197], Valid [Acc (TTA): 0.9177], LR: 0.000259\n",
            "EPOCH: 340, Train [Acc: 0.9210], Valid [Acc (TTA): 0.9181], LR: 0.000232\n",
            "EPOCH: 350, Train [Acc: 0.9230], Valid [Acc (TTA): 0.9169], LR: 0.000206\n",
            "EPOCH: 360, Train [Acc: 0.9226], Valid [Acc (TTA): 0.9192], LR: 0.000181\n",
            "EPOCH: 370, Train [Acc: 0.9237], Valid [Acc (TTA): 0.9198], LR: 0.000158\n",
            "EPOCH: 380, Train [Acc: 0.9257], Valid [Acc (TTA): 0.9226], LR: 0.000136\n",
            "    -> Best Valid Accuracy Updated: 0.9226\n",
            "EPOCH: 390, Train [Acc: 0.9257], Valid [Acc (TTA): 0.9200], LR: 0.000115\n",
            "EPOCH: 400, Train [Acc: 0.9280], Valid [Acc (TTA): 0.9217], LR: 0.000095\n",
            "EPOCH: 410, Train [Acc: 0.9276], Valid [Acc (TTA): 0.9226], LR: 0.000078\n",
            "EPOCH: 420, Train [Acc: 0.9284], Valid [Acc (TTA): 0.9203], LR: 0.000062\n",
            "EPOCH: 430, Train [Acc: 0.9285], Valid [Acc (TTA): 0.9215], LR: 0.000048\n",
            "EPOCH: 440, Train [Acc: 0.9330], Valid [Acc (TTA): 0.9198], LR: 0.000035\n",
            "EPOCH: 450, Train [Acc: 0.9324], Valid [Acc (TTA): 0.9219], LR: 0.000024\n",
            "EPOCH: 460, Train [Acc: 0.9320], Valid [Acc (TTA): 0.9207], LR: 0.000016\n",
            "EPOCH: 470, Train [Acc: 0.9318], Valid [Acc (TTA): 0.9213], LR: 0.000009\n",
            "EPOCH: 480, Train [Acc: 0.9315], Valid [Acc (TTA): 0.9212], LR: 0.000004\n",
            "\n",
            "--- Early stopping triggered after 487 epochs. ---\n",
            "--- モデル 2 学習完了 (Time: 2652.9 sec) ---\n",
            "    Best Valid Accuracy (TTA): 0.9229\n",
            "\n",
            "--- モデル 3/3 (Seed: 555) の学習開始 ---\n",
            "EPOCH:   1, Train [Acc: 0.7615], Valid [Acc (TTA): 0.8133], LR: 0.001000\n",
            "    -> Best Valid Accuracy Updated: 0.8133\n",
            "EPOCH:  10, Train [Acc: 0.8452], Valid [Acc (TTA): 0.8731], LR: 0.000999\n",
            "    -> Best Valid Accuracy Updated: 0.8731\n",
            "EPOCH:  20, Train [Acc: 0.8620], Valid [Acc (TTA): 0.8856], LR: 0.000996\n",
            "    -> Best Valid Accuracy Updated: 0.8856\n",
            "EPOCH:  30, Train [Acc: 0.8683], Valid [Acc (TTA): 0.8896], LR: 0.000991\n",
            "    -> Best Valid Accuracy Updated: 0.8896\n",
            "EPOCH:  40, Train [Acc: 0.8713], Valid [Acc (TTA): 0.8898], LR: 0.000984\n",
            "EPOCH:  50, Train [Acc: 0.8763], Valid [Acc (TTA): 0.8969], LR: 0.000976\n",
            "    -> Best Valid Accuracy Updated: 0.8969\n",
            "EPOCH:  60, Train [Acc: 0.8787], Valid [Acc (TTA): 0.8983], LR: 0.000965\n",
            "    -> Best Valid Accuracy Updated: 0.8983\n",
            "EPOCH:  70, Train [Acc: 0.8818], Valid [Acc (TTA): 0.9010], LR: 0.000952\n",
            "    -> Best Valid Accuracy Updated: 0.9010\n",
            "EPOCH:  80, Train [Acc: 0.8843], Valid [Acc (TTA): 0.9047], LR: 0.000938\n",
            "    -> Best Valid Accuracy Updated: 0.9047\n",
            "EPOCH:  90, Train [Acc: 0.8845], Valid [Acc (TTA): 0.9030], LR: 0.000922\n",
            "EPOCH: 100, Train [Acc: 0.8870], Valid [Acc (TTA): 0.9037], LR: 0.000905\n",
            "EPOCH: 110, Train [Acc: 0.8861], Valid [Acc (TTA): 0.9029], LR: 0.000885\n",
            "EPOCH: 120, Train [Acc: 0.8892], Valid [Acc (TTA): 0.9033], LR: 0.000864\n",
            "EPOCH: 130, Train [Acc: 0.8911], Valid [Acc (TTA): 0.9071], LR: 0.000842\n",
            "EPOCH: 140, Train [Acc: 0.8956], Valid [Acc (TTA): 0.9089], LR: 0.000819\n",
            "EPOCH: 150, Train [Acc: 0.8946], Valid [Acc (TTA): 0.9095], LR: 0.000794\n",
            "EPOCH: 160, Train [Acc: 0.8967], Valid [Acc (TTA): 0.9095], LR: 0.000768\n",
            "EPOCH: 170, Train [Acc: 0.8977], Valid [Acc (TTA): 0.9107], LR: 0.000741\n",
            "EPOCH: 180, Train [Acc: 0.8994], Valid [Acc (TTA): 0.9067], LR: 0.000713\n",
            "EPOCH: 190, Train [Acc: 0.9001], Valid [Acc (TTA): 0.9110], LR: 0.000684\n",
            "EPOCH: 200, Train [Acc: 0.9018], Valid [Acc (TTA): 0.9123], LR: 0.000655\n",
            "EPOCH: 210, Train [Acc: 0.9021], Valid [Acc (TTA): 0.9127], LR: 0.000624\n",
            "EPOCH: 220, Train [Acc: 0.9047], Valid [Acc (TTA): 0.9133], LR: 0.000594\n",
            "EPOCH: 230, Train [Acc: 0.9061], Valid [Acc (TTA): 0.9106], LR: 0.000563\n",
            "EPOCH: 240, Train [Acc: 0.9072], Valid [Acc (TTA): 0.9117], LR: 0.000531\n",
            "EPOCH: 250, Train [Acc: 0.9078], Valid [Acc (TTA): 0.9173], LR: 0.000500\n",
            "    -> Best Valid Accuracy Updated: 0.9173\n",
            "EPOCH: 260, Train [Acc: 0.9096], Valid [Acc (TTA): 0.9165], LR: 0.000469\n",
            "EPOCH: 270, Train [Acc: 0.9110], Valid [Acc (TTA): 0.9173], LR: 0.000437\n",
            "EPOCH: 280, Train [Acc: 0.9138], Valid [Acc (TTA): 0.9153], LR: 0.000406\n",
            "EPOCH: 290, Train [Acc: 0.9136], Valid [Acc (TTA): 0.9130], LR: 0.000376\n",
            "EPOCH: 300, Train [Acc: 0.9139], Valid [Acc (TTA): 0.9167], LR: 0.000345\n",
            "EPOCH: 310, Train [Acc: 0.9155], Valid [Acc (TTA): 0.9167], LR: 0.000316\n",
            "EPOCH: 320, Train [Acc: 0.9158], Valid [Acc (TTA): 0.9171], LR: 0.000287\n",
            "EPOCH: 330, Train [Acc: 0.9205], Valid [Acc (TTA): 0.9177], LR: 0.000259\n",
            "EPOCH: 340, Train [Acc: 0.9204], Valid [Acc (TTA): 0.9171], LR: 0.000232\n",
            "EPOCH: 350, Train [Acc: 0.9200], Valid [Acc (TTA): 0.9201], LR: 0.000206\n",
            "EPOCH: 360, Train [Acc: 0.9236], Valid [Acc (TTA): 0.9183], LR: 0.000181\n",
            "EPOCH: 370, Train [Acc: 0.9243], Valid [Acc (TTA): 0.9180], LR: 0.000158\n",
            "EPOCH: 380, Train [Acc: 0.9284], Valid [Acc (TTA): 0.9205], LR: 0.000136\n",
            "EPOCH: 390, Train [Acc: 0.9282], Valid [Acc (TTA): 0.9197], LR: 0.000115\n",
            "EPOCH: 400, Train [Acc: 0.9275], Valid [Acc (TTA): 0.9204], LR: 0.000095\n",
            "EPOCH: 410, Train [Acc: 0.9308], Valid [Acc (TTA): 0.9215], LR: 0.000078\n",
            "EPOCH: 420, Train [Acc: 0.9293], Valid [Acc (TTA): 0.9212], LR: 0.000062\n",
            "EPOCH: 430, Train [Acc: 0.9309], Valid [Acc (TTA): 0.9215], LR: 0.000048\n",
            "EPOCH: 440, Train [Acc: 0.9302], Valid [Acc (TTA): 0.9197], LR: 0.000035\n",
            "EPOCH: 450, Train [Acc: 0.9328], Valid [Acc (TTA): 0.9212], LR: 0.000024\n",
            "EPOCH: 460, Train [Acc: 0.9317], Valid [Acc (TTA): 0.9201], LR: 0.000016\n",
            "EPOCH: 470, Train [Acc: 0.9323], Valid [Acc (TTA): 0.9203], LR: 0.000009\n",
            "EPOCH: 480, Train [Acc: 0.9319], Valid [Acc (TTA): 0.9209], LR: 0.000004\n",
            "\n",
            "--- Early stopping triggered after 489 epochs. ---\n",
            "--- モデル 3 学習完了 (Time: 2667.9 sec) ---\n",
            "    Best Valid Accuracy (TTA): 0.9224\n",
            "\n",
            "--- 全モデルの学習完了 ---\n",
            "--- アンサンブルによる最終予測 (TTA + Ensemble) ---\n",
            "\n",
            "--- 処理完了 (Total Time: 8046.7 sec) ---\n",
            "最適化された予測ファイルが保存されました: /content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab/Lecture04/submission_pred_04_optimized_final.csv\n",
            "目標スコア 0.95 の達成を期待します。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 0. 環境設定とAPI制限 (★「コードB」の安全なAPI制限を導入)\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Google Driveマウントと作業ディレクトリ設定 ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # ★★★ ご自身の環境に合わせて修正してください ★★★\n",
        "    work_dir = '/content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab'\n",
        "    if not os.path.exists(work_dir):\n",
        "        print(f\"警告: ディレクトリ '{work_dir}' が存在しません。'.' を使用します。\")\n",
        "        work_dir = '.'\n",
        "except ImportError:\n",
        "    print(\"Google Colab 環境ではありません。カレントディレクトリ '.' を使用します。\")\n",
        "    work_dir = '.'\n",
        "print(f\"作業ディレクトリ: {work_dir}\")\n",
        "\n",
        "\n",
        "# ★★★ 改善点 1: 「コードB」で使われていた安全なAPI制限に変更 ★★★\n",
        "# optim.Adamが依存する内部モジュール(functional等)を削除しません。\n",
        "nn_except = [\"Module\", \"Parameter\", \"Sequential\", \"parameter\", \"init\", \"CrossEntropyLoss\", \"ModuleList\", \"ModuleDict\"] # Added ModuleDict\n",
        "\n",
        "for m in inspect.getmembers(nn):\n",
        "    attr_name = m[0]\n",
        "    attr_value = m[1]\n",
        "\n",
        "    # 除外リストまたはプライベート属性はスキップ\n",
        "    if attr_name in nn_except or attr_name.startswith(\"__\"):\n",
        "        continue\n",
        "\n",
        "    # ★重要: モジュール型の属性は削除しない (functional, attention等)\n",
        "    if inspect.ismodule(attr_value):\n",
        "        continue\n",
        "\n",
        "    # それ以外のクラス・関数(nn.Linear, nn.ReLUなど)を削除\n",
        "    try:\n",
        "        delattr(nn, attr_name)\n",
        "    except (AttributeError, TypeError):\n",
        "        pass\n",
        "\n",
        "print(f\"--- API制限適用完了 (許可: {nn_except} + サブモジュール) ---\")\n",
        "# ★★★ 改善完了 ★★★\n",
        "\n",
        "\n",
        "# --- シード固定関数 ---\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) # アンサンブルのためにallも設定\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- デバイス設定 ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 1. データ準備 (★改善点 2: Cutoutのバグを今度こそ修正)\n",
        "# ===================================================================\n",
        "\n",
        "def augment_data_torch_cutout(images_flat, h_flip_prob=0.5, shift_pixels=2, cutout_size=8):\n",
        "    \"\"\"\n",
        "    torchvisionに依存しない、テンソル演算のみによるデータ拡張。\n",
        "    \"\"\"\n",
        "    images_2d = images_flat.reshape(-1, 1, 28, 28)\n",
        "    n_images, c, h, w = images_2d.shape\n",
        "\n",
        "    # --- 1. 水平フリップ (p=0.5) ---\n",
        "    flip_mask = torch.rand(n_images, device=images_flat.device) < h_flip_prob\n",
        "    if flip_mask.any():\n",
        "        images_2d[flip_mask] = torch.flip(images_2d[flip_mask], dims=[3])\n",
        "\n",
        "    # --- 2. ランダムシフト (±2ピクセル) ---\n",
        "    shifts = torch.randint(-shift_pixels, shift_pixels + 1, (n_images, 2), device=images_flat.device)\n",
        "    # nn.functional.pad は安全なAPI制限により削除されていないため使用可能\n",
        "    padded_images = torch.nn.functional.pad(images_2d, (shift_pixels, shift_pixels, shift_pixels, shift_pixels), mode='constant', value=0.0)\n",
        "    augmented_shifted = torch.empty_like(images_2d)\n",
        "\n",
        "    for i in range(n_images):\n",
        "        shift_y, shift_x = shifts[i, 0].item(), shifts[i, 1].item()\n",
        "        y_start = shift_pixels + shift_y\n",
        "        x_start = shift_pixels + shift_x\n",
        "        augmented_shifted[i] = padded_images[i, :, y_start:y_start+h, x_start:x_start+w]\n",
        "\n",
        "    images_2d = augmented_shifted\n",
        "\n",
        "    # --- 3. カットアウト (Cutout) ---\n",
        "    cutout_half = cutout_size // 2\n",
        "    center_y = torch.randint(0, h, (n_images,), device=images_flat.device)\n",
        "    center_x = torch.randint(0, w, (n_images,), device=images_flat.device)\n",
        "\n",
        "    for i in range(n_images):\n",
        "        y1 = torch.clamp(center_y[i] - cutout_half, 0, h).item()\n",
        "        y2 = torch.clamp(center_y[i] + cutout_half, 0, h).item()\n",
        "\n",
        "        # ★★★ 改善点 2: バグ修正 ★★★\n",
        "        # x1 の計算が両方 '+' になっていたのを '-' に修正\n",
        "        # x1 = torch.clamp(center_x[i] + cutout_half, 0, w).item() # <- バグ\n",
        "        x1 = torch.clamp(center_x[i] - cutout_half, 0, w).item() # <- 修正！\n",
        "        x2 = torch.clamp(center_x[i] + cutout_half, 0, w).item()\n",
        "\n",
        "        if y1 < y2 and x1 < x2:\n",
        "            images_2d[i, :, y1:y2, x1:x2] = 0.0\n",
        "    # ★★★ 改善完了 ★★★\n",
        "\n",
        "    return images_2d.reshape(-1, 784)\n",
        "\n",
        "# --- データセットクラス (変更なし) ---\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, x_data, t_label):\n",
        "        # ★改善: 0-1に正規化 (augment_data_torch_cutoutは0-1を前提)\n",
        "        self.x_data = x_data.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "        self.t_label = t_label\n",
        "        self.t_hot = np.eye(10)[t_label].astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.t_label[idx], dtype=torch.long)\n",
        "        hot_label = torch.tensor(self.t_hot[idx], dtype=torch.float32)\n",
        "        return img_tensor, hot_label, label\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, x_data):\n",
        "        self.x_data = x_data.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_tensor = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
        "        return img_tensor\n",
        "\n",
        "# --- データのロードと分割 ---\n",
        "data_path = os.path.join(work_dir, 'Lecture04', 'data')\n",
        "x_train_all = np.load(os.path.join(data_path, 'x_train.npy'))\n",
        "t_train_all = np.load(os.path.join(data_path, 'y_train.npy'))\n",
        "x_test = np.load(os.path.join(data_path, 'x_test.npy'))\n",
        "\n",
        "x_train_split, x_valid_split, t_train_split_label, t_valid_split_label = train_test_split(\n",
        "    x_train_all, t_train_all, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"訓練データ: {x_train_split.shape}, 検証データ: {x_valid_split.shape}\")\n",
        "\n",
        "# --- データセットインスタンスの作成 ---\n",
        "train_data = FashionDataset(x_train_split, t_train_split_label)\n",
        "valid_data = FashionDataset(x_valid_split, t_valid_split_label)\n",
        "test_data = TestDataset(x_test)\n",
        "\n",
        "\n",
        "# --- データローダーの作成 ---\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "\n",
        "dataloader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "dataloader_valid = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "dataloader_test = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(\"--- データローダー準備完了 (torchvision非依存) ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. PyTorchによるニューラルネットワークのクラス化 (変更なし)\n",
        "# ===================================================================\n",
        "\n",
        "def relu(x):\n",
        "    return torch.where(x > 0, x, torch.zeros_like(x))\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, is_relu=True):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty(in_dim, out_dim, dtype=torch.float32))\n",
        "        self.b = nn.Parameter(torch.empty(out_dim, dtype=torch.float32))\n",
        "        self.initialize_parameters(is_relu)\n",
        "\n",
        "    def initialize_parameters(self, is_relu):\n",
        "        if is_relu:\n",
        "            nn.init.kaiming_normal_(self.W, mode='fan_in', nonlinearity='relu')\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.W)\n",
        "        nn.init.zeros_(self.b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.W) + self.b\n",
        "\n",
        "class BatchNorm1d(nn.Module):\n",
        "    def __init__(self, dim, momentum=0.9, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim).float())\n",
        "        self.beta = nn.Parameter(torch.zeros(dim).float())\n",
        "        self.register_buffer('running_mean', torch.zeros(dim).float())\n",
        "        self.register_buffer('running_var', torch.ones(dim).float())\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            if x.shape[0] > 1:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = x.var(axis=0, unbiased=False)\n",
        "            else:\n",
        "                mu = x.mean(axis=0)\n",
        "                var = torch.zeros_like(mu)\n",
        "            self.running_mean = self.momentum * self.running_mean.detach() + (1.0 - self.momentum) * mu.detach()\n",
        "            self.running_var = self.momentum * self.running_var.detach() + (1.0 - self.momentum) * var.detach()\n",
        "            x_norm = (x - mu) / torch.sqrt(var + self.eps)\n",
        "        else:\n",
        "            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "class Dropout(nn.Module):\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.dropout_ratio > 0.0:\n",
        "            drop_mask = (torch.rand_like(x) > self.dropout_ratio).float() / (1.0 - self.dropout_ratio)\n",
        "            return x * drop_mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum):\n",
        "        super().__init__()\n",
        "        self.fc1 = Dense(in_dim, hidden_sizes[0], is_relu=True)\n",
        "        self.bn1 = BatchNorm1d(hidden_sizes[0], momentum=bn_momentum)\n",
        "        self.drop1 = Dropout(dropout_ratio)\n",
        "        self.fc2 = Dense(hidden_sizes[0], hidden_sizes[1], is_relu=True)\n",
        "        self.bn2 = BatchNorm1d(hidden_sizes[1], momentum=bn_momentum)\n",
        "        self.drop2 = Dropout(dropout_ratio)\n",
        "        self.fc3 = Dense(hidden_sizes[1], hidden_sizes[2], is_relu=True)\n",
        "        self.bn3 = BatchNorm1d(hidden_sizes[2], momentum=bn_momentum)\n",
        "        self.drop3 = Dropout(dropout_ratio)\n",
        "        self.output_layer = Dense(hidden_sizes[2], out_dim, is_relu=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = relu(self.drop1(self.bn1(self.fc1(x))))\n",
        "        x = relu(self.drop2(self.bn2(self.fc2(x))))\n",
        "        x = relu(self.drop3(self.bn3(self.fc3(x))))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. アンサンブル学習ループ (★改善点 3: dropout_ratio=0.3)\n",
        "# ===================================================================\n",
        "\n",
        "in_dim = 784\n",
        "out_dim = 10\n",
        "hidden_sizes = [1024, 512, 256]\n",
        "bn_momentum = 0.9\n",
        "lr = 0.001\n",
        "reg_lambda = 1e-5\n",
        "\n",
        "# ★★★ 改善点 3: 正則化のバランス調整 ★★★\n",
        "# 学習不足(Underfitting)を解消するため、Dropoutを 0.4 から 0.3 に戻します。\n",
        "dropout_ratio = 0.3 # 0.4\n",
        "# ★★★ 改善完了 ★★★\n",
        "\n",
        "n_epochs = 500\n",
        "patience = 60\n",
        "\n",
        "ensemble_seeds = [42, 123, 555]\n",
        "all_best_model_states = []\n",
        "all_best_valid_accs = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"--- アンサンブル学習開始 (Models: {len(ensemble_seeds)}, Epochs: {n_epochs}, Dropout: {dropout_ratio}) ---\")\n",
        "print(\"--- ★データ拡張: 手動Cutout + Shift + Flip (torchvision非依存) ---\")\n",
        "\n",
        "for i, seed in enumerate(ensemble_seeds):\n",
        "    print(f\"\\n--- モデル {i+1}/{len(ensemble_seeds)} (Seed: {seed}) の学習開始 ---\")\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    mlp = MLP(in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum).to(device)\n",
        "\n",
        "    # ★★★ エラーが発生していた行 ★★★\n",
        "    # 安全なAPI制限に変更したため、optim.Adam は\n",
        "    # nn.functional や nn.ModuleDict を見つけることができ、正常に初期化されます。\n",
        "    optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=reg_lambda)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
        "\n",
        "    best_valid_accuracy = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        losses_train = []\n",
        "        train_num = 0\n",
        "        train_true_num = 0\n",
        "\n",
        "        mlp.train()\n",
        "        for x, t_hot, t_label in dataloader_train:\n",
        "            x, t_label = x.to(device), t_label.to(device)\n",
        "\n",
        "            # --- ★手動データ拡張 (バグ修正済みCutout) ---\n",
        "            x_aug = augment_data_torch_cutout(x,\n",
        "                                                h_flip_prob=0.5,\n",
        "                                                shift_pixels=2,\n",
        "                                                cutout_size=8)\n",
        "\n",
        "            y_logits = mlp(x_aug)\n",
        "            loss = criterion(y_logits, t_label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses_train.append(loss.tolist())\n",
        "            pred = y_logits.argmax(1)\n",
        "            acc = (pred == t_label).float()\n",
        "            train_num += acc.size(0)\n",
        "            train_true_num += acc.sum().item()\n",
        "\n",
        "        # --- 検証 (★手動TTA: 水平フリップ) ---\n",
        "        mlp.eval()\n",
        "        losses_valid = []\n",
        "        valid_num = 0\n",
        "        valid_true_num = 0\n",
        "        with torch.no_grad():\n",
        "            for x, t_hot, t_label in dataloader_valid:\n",
        "                x, t_label = x.to(device), t_label.to(device)\n",
        "\n",
        "                y_logits_original = mlp(x)\n",
        "\n",
        "                x_reshaped = x.reshape(-1, 1, 28, 28)\n",
        "                x_flipped = torch.flip(x_reshaped, dims=[3])\n",
        "                x_flipped_flat = x_flipped.reshape(-1, 784)\n",
        "                y_logits_flipped = mlp.forward(x_flipped_flat)\n",
        "\n",
        "                y_logits_avg = (y_logits_original + y_logits_flipped) / 2.0\n",
        "\n",
        "                loss = criterion(y_logits_avg, t_label)\n",
        "                losses_valid.append(loss.tolist())\n",
        "\n",
        "                pred = y_logits_avg.argmax(1)\n",
        "                acc = (pred == t_label).float()\n",
        "                valid_num += acc.size(0)\n",
        "                valid_true_num += acc.sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        current_train_accuracy = train_true_num / train_num\n",
        "        current_valid_accuracy = valid_true_num / valid_num\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f'EPOCH: {epoch+1:3d}, Train [Loss: {np.mean(losses_train):.4f}, Acc: {current_train_accuracy:.4f}], '\n",
        "                  f'Valid [Loss: {np.mean(losses_valid):.4f}, Acc(TTA): {current_valid_accuracy:.4f}], '\n",
        "                  f'LR: {current_lr:.6f}')\n",
        "\n",
        "        if current_valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = current_valid_accuracy\n",
        "            patience_counter = 0\n",
        "            best_model_state = copy.deepcopy(mlp.state_dict())\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f\"    -> Best TTA Valid Accuracy Updated: {best_valid_accuracy:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n--- Early stopping triggered after {epoch+1} epochs. ---\")\n",
        "            break\n",
        "\n",
        "    all_best_model_states.append(best_model_state)\n",
        "    all_best_valid_accs.append(best_valid_accuracy)\n",
        "    print(f\"--- モデル {i+1} 完了 (Best Valid Acc: {best_valid_accuracy:.4f}) ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 最終予測 (アンサンブルと手動TTA) (変更なし)\n",
        "# ===================================================================\n",
        "print(\"\\n--- 全モデルの学習完了 ---\")\n",
        "print(f\"各モデルのベスト検証精度 (TTA): {[round(acc, 4) for acc in all_best_valid_accs]}\")\n",
        "print(f\"平均検証精度 (TTA): {np.mean(all_best_valid_accs):.4f}\")\n",
        "print(\"\\n--- アンサンブルとTTAによる最終予測の生成 ---\")\n",
        "\n",
        "all_test_logits = []\n",
        "\n",
        "mlp = MLP(in_dim, hidden_sizes, out_dim, dropout_ratio, bn_momentum).to(device)\n",
        "mlp.eval()\n",
        "\n",
        "for model_state in all_best_model_states:\n",
        "    if model_state is None:\n",
        "        print(\"警告: 訓練されなかったモデル（stateがNone）をスキップします。\")\n",
        "        continue\n",
        "\n",
        "    mlp.load_state_dict(model_state)\n",
        "\n",
        "    model_test_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in dataloader_test:\n",
        "            x = x.to(device)\n",
        "\n",
        "            y_logits_original = mlp.forward(x)\n",
        "\n",
        "            x_reshaped = x.reshape(-1, 1, 28, 28)\n",
        "            x_flipped = torch.flip(x_reshaped, dims=[3])\n",
        "            x_flipped_flat = x_flipped.reshape(-1, 784)\n",
        "            y_logits_flipped = mlp.forward(x_flipped_flat)\n",
        "\n",
        "            y_logits_avg = (y_logits_original + y_logits_flipped) / 2.0\n",
        "\n",
        "            model_test_logits.append(y_logits_avg.cpu())\n",
        "\n",
        "    all_test_logits.append(torch.cat(model_test_logits, dim=0))\n",
        "\n",
        "if not all_test_logits:\n",
        "    print(\"エラー: 有効なモデルが1つも訓練されませんでした。\")\n",
        "else:\n",
        "    stacked_logits = torch.stack(all_test_logits, dim=0)\n",
        "    mean_logits = torch.mean(stacked_logits, dim=0)\n",
        "    final_pred = mean_logits.argmax(1).tolist()\n",
        "\n",
        "    # --- 提出ファイルの保存 ---\n",
        "    submission = pd.Series(final_pred, name='label')\n",
        "    submission_filename = os.path.join(work_dir, 'Lecture04', 'submission_pred_11_final_fixed.csv')\n",
        "    os.makedirs(os.path.dirname(submission_filename), exist_ok=True)\n",
        "    submission.to_csv(submission_filename, header=True, index_label='id')\n",
        "    print(f\"\\n--- 予測完了。提出ファイル '{submission_filename}' を保存しました。 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipDPi-zYXL9a",
        "outputId": "5ce496ee-71ae-4f7b-88d4-543f5c560e71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "作業ディレクトリ: /content/drive/MyDrive/Colab Notebooks/DLBasics2025_colab\n",
            "--- API制限適用完了 (許可: ['Module', 'Parameter', 'Sequential', 'parameter', 'init', 'CrossEntropyLoss', 'ModuleList', 'ModuleDict'] + サブモジュール) ---\n",
            "使用デバイス: cuda\n",
            "訓練データ: (48000, 28, 28), 検証データ: (12000, 28, 28)\n",
            "--- データローダー準備完了 (torchvision非依存) ---\n",
            "--- アンサンブル学習開始 (Models: 3, Epochs: 500, Dropout: 0.3) ---\n",
            "--- ★データ拡張: 手動Cutout + Shift + Flip (torchvision非依存) ---\n",
            "\n",
            "--- モデル 1/3 (Seed: 42) の学習開始 ---\n",
            "EPOCH:   1, Train [Loss: 0.9033, Acc: 0.6624], Valid [Loss: 0.5901, Acc(TTA): 0.7642], LR: 0.001000\n",
            "    -> Best TTA Valid Accuracy Updated: 0.7642\n",
            "EPOCH:  10, Train [Loss: 0.5509, Acc: 0.7960], Valid [Loss: 0.4017, Acc(TTA): 0.8488], LR: 0.000999\n",
            "    -> Best TTA Valid Accuracy Updated: 0.8488\n",
            "EPOCH:  20, Train [Loss: 0.4875, Acc: 0.8183], Valid [Loss: 0.3569, Acc(TTA): 0.8650], LR: 0.000996\n",
            "    -> Best TTA Valid Accuracy Updated: 0.8650\n",
            "EPOCH:  30, Train [Loss: 0.4586, Acc: 0.8300], Valid [Loss: 0.3399, Acc(TTA): 0.8696], LR: 0.000991\n",
            "EPOCH:  40, Train [Loss: 0.4435, Acc: 0.8355], Valid [Loss: 0.3213, Acc(TTA): 0.8766], LR: 0.000984\n",
            "EPOCH:  50, Train [Loss: 0.4294, Acc: 0.8402], Valid [Loss: 0.3199, Acc(TTA): 0.8776], LR: 0.000976\n",
            "EPOCH:  60, Train [Loss: 0.4203, Acc: 0.8410], Valid [Loss: 0.3150, Acc(TTA): 0.8781], LR: 0.000965\n",
            "EPOCH:  70, Train [Loss: 0.4147, Acc: 0.8445], Valid [Loss: 0.3033, Acc(TTA): 0.8875], LR: 0.000952\n",
            "    -> Best TTA Valid Accuracy Updated: 0.8875\n",
            "EPOCH:  80, Train [Loss: 0.4029, Acc: 0.8486], Valid [Loss: 0.2990, Acc(TTA): 0.8861], LR: 0.000938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a936b79fec0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1391369365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# --- ★手動データ拡張 (バグ修正済みCutout) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             x_aug = augment_data_torch_cutout(x,\n\u001b[0m\u001b[1;32m    320\u001b[0m                                                 \u001b[0mh_flip_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                                                 \u001b[0mshift_pixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1391369365.py\u001b[0m in \u001b[0;36maugment_data_torch_cutout\u001b[0;34m(images_flat, h_flip_prob, shift_pixels, cutout_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# x1 の計算が両方 '+' になっていたのを '-' に修正\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# x1 = torch.clamp(center_x[i] + cutout_half, 0, w).item() # <- バグ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcutout_half\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <- 修正！\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcutout_half\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "claude"
      ],
      "metadata": {
        "id": "MIlFwQolYGos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== セットアップ（最初に1回だけ実行）==================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import inspect\n",
        "\n",
        "# 使用できるAPIを制限（改良版: サブモジュールは保護）\n",
        "nn_except = [\"Module\", \"Parameter\", \"Sequential\", \"parameter\", \"ModuleList\"] # Added ModuleList\n",
        "# サブモジュール（削除してはいけない）\n",
        "nn_submodules = [\"attention\", \"functional\", \"init\", \"modules\", \"utils\", \"common_types\", \"grad\"]\n",
        "\n",
        "for m in inspect.getmembers(nn):\n",
        "    attr_name = m[0]\n",
        "    # 除外リスト、プライベート属性、サブモジュールはスキップ\n",
        "    if attr_name in nn_except or attr_name[0:2] == \"__\" or attr_name in nn_submodules:\n",
        "        continue\n",
        "    # モジュール型もスキップ\n",
        "    if inspect.ismodule(m[1]):\n",
        "        continue\n",
        "    # それ以外を削除\n",
        "    try:\n",
        "        delattr(nn, attr_name)\n",
        "    except (AttributeError, TypeError):\n",
        "        pass  # 削除できない場合はスキップ\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 学習データ\n",
        "x_train = np.load(work_dir + '/Lecture04/data/x_train.npy')\n",
        "t_train = np.load(work_dir + '/Lecture04/data/y_train.npy')\n",
        "\n",
        "# テストデータ\n",
        "x_test = np.load(work_dir + '/Lecture04/data/x_test.npy')\n",
        "\n",
        "class train_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_train, t_train):\n",
        "        self.x_train = x_train.reshape(-1, 784).astype('float32') / 255\n",
        "        self.t_train = t_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x_train[idx], dtype=torch.float), torch.tensor(self.t_train[idx], dtype=torch.long)\n",
        "\n",
        "class test_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_test):\n",
        "        self.x_test = x_test.reshape(-1, 784).astype('float32') / 255\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_test.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x_test[idx], dtype=torch.float)\n",
        "\n",
        "trainval_data = train_dataset(x_train, t_train)\n",
        "test_data = test_dataset(x_test)\n",
        "print(f\"Train+Val samples: {len(trainval_data)}, Test samples: {len(test_data)}\")\n",
        "\n",
        "\n",
        "# ================== カスタムレイヤの実装 ==================\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"ReLU活性化関数\"\"\"\n",
        "    return torch.where(x > 0, x, torch.zeros_like(x))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Softmax関数\"\"\"\n",
        "    exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
        "    return exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "class CustomDense(nn.Module):\n",
        "    \"\"\"He初期化を使ったカスタム全結合層\"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(CustomDense, self).__init__()\n",
        "        # He初期化（Kaiming初期化）\n",
        "        std = torch.sqrt(torch.tensor(2.0 / in_features))\n",
        "        self.W = nn.Parameter(torch.randn(in_features, out_features) * std)\n",
        "        self.b = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.W) + self.b\n",
        "\n",
        "\n",
        "class CustomBatchNorm1d(nn.Module):\n",
        "    \"\"\"カスタムBatch Normalizationレイヤ\"\"\"\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(CustomBatchNorm1d, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # 学習可能なパラメータ\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "        # 推論時用の統計量（学習されない）\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            # 訓練時: バッチの統計量を使用\n",
        "            batch_mean = torch.mean(x, dim=0)\n",
        "            batch_var = torch.var(x, dim=0, unbiased=False)\n",
        "\n",
        "            # 移動平均を更新\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            # 正規化\n",
        "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
        "        else:\n",
        "            # 推論時: 移動平均を使用\n",
        "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "\n",
        "        # スケールとシフト\n",
        "        return self.gamma * x_normalized + self.beta\n",
        "\n",
        "\n",
        "class CustomDropout(nn.Module):\n",
        "    \"\"\"カスタムDropoutレイヤ\"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        super(CustomDropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            # 訓練時: ドロップアウトを適用\n",
        "            mask = (torch.rand_like(x) > self.p).float()\n",
        "            return x * mask / (1 - self.p)  # スケーリング\n",
        "        else:\n",
        "            # 推論時: 何もしない\n",
        "            return x\n",
        "\n",
        "\n",
        "class CustomReLU(nn.Module):\n",
        "    \"\"\"カスタムReLUレイヤ\"\"\"\n",
        "    def forward(self, x):\n",
        "        return relu(x)\n",
        "\n",
        "\n",
        "# ================== データ拡張関数（テンソルスライスのみ）==================\n",
        "\n",
        "def augment_data(x, shift_range=2):\n",
        "    \"\"\"\n",
        "    データ拡張: ランダム左右反転とランダムシフト\n",
        "    nn.functionalを使わず、テンソルのスライス操作のみで実装\n",
        "    x: shape (batch_size, 784)\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    x_images = x.view(batch_size, 28, 28)\n",
        "    x_augmented = torch.zeros_like(x_images)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img = x_images[i]\n",
        "\n",
        "        # ランダム左右反転（50%の確率）\n",
        "        if torch.rand(1).item() > 0.5:\n",
        "            img = torch.flip(img, dims=[1])\n",
        "\n",
        "        # ランダムシフト（テンソルスライスで手動実装）\n",
        "        if shift_range > 0:\n",
        "            shift_h = torch.randint(-shift_range, shift_range + 1, (1,)).item()\n",
        "            shift_w = torch.randint(-shift_range, shift_range + 1, (1,)).item()\n",
        "\n",
        "            # シフト後の画像を作成（ゼロパディング）\n",
        "            shifted_img = torch.zeros_like(img)\n",
        "\n",
        "            # コピー元の範囲を計算\n",
        "            src_h_start = max(0, -shift_h)\n",
        "            src_h_end = min(28, 28 - shift_h)\n",
        "            src_w_start = max(0, -shift_w)\n",
        "            src_w_end = min(28, 28 - shift_w)\n",
        "\n",
        "            # コピー先の範囲を計算\n",
        "            dst_h_start = max(0, shift_h)\n",
        "            dst_h_end = dst_h_start + (src_h_end - src_h_start)\n",
        "            dst_w_start = max(0, shift_w)\n",
        "            dst_w_end = dst_w_start + (src_w_end - src_w_start)\n",
        "\n",
        "            # スライスでコピー\n",
        "            shifted_img[dst_h_start:dst_h_end, dst_w_start:dst_w_end] = \\\n",
        "                img[src_h_start:src_h_end, src_w_start:src_w_end]\n",
        "\n",
        "            img = shifted_img\n",
        "\n",
        "        x_augmented[i] = img\n",
        "\n",
        "    return x_augmented.view(batch_size, 784)\n",
        "\n",
        "\n",
        "# ================== MLPモデルの構築 ==================\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"深層MLP\"\"\"\n",
        "    def __init__(self, in_dim, hid_dims, out_dim, dropout_p=0.3):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = in_dim\n",
        "\n",
        "        # 隠れ層\n",
        "        for hid_dim in hid_dims:\n",
        "            layers.append(CustomDense(prev_dim, hid_dim))\n",
        "            layers.append(CustomBatchNorm1d(hid_dim))\n",
        "            layers.append(CustomReLU())\n",
        "            layers.append(CustomDropout(dropout_p))\n",
        "            prev_dim = hid_dim\n",
        "\n",
        "        # 出力層\n",
        "        layers.append(CustomDense(prev_dim, out_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ================== ハイパーパラメータ設定 ==================\n",
        "\n",
        "batch_size = 128\n",
        "val_size = 10000\n",
        "train_size = len(trainval_data) - val_size\n",
        "\n",
        "train_data, val_data = torch.utils.data.random_split(trainval_data, [train_size, val_size])\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    val_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# モデルのパラメータ\n",
        "in_dim = 784\n",
        "hid_dims = [1024, 512, 256, 128]  # 4層の隠れ層\n",
        "out_dim = 10\n",
        "dropout_p = 0.4\n",
        "\n",
        "# 学習パラメータ\n",
        "lr = 0.001\n",
        "weight_decay = 1e-4\n",
        "n_epochs = 400\n",
        "\n",
        "# モデルとオプティマイザの初期化\n",
        "mlp = MLP(in_dim, hid_dims, out_dim, dropout_p).to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# コサインアニーリング学習率スケジューラ\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
        "\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(f\"Hidden layers: {hid_dims}\")\n",
        "print(f\"Dropout: {dropout_p}\")\n",
        "print(f\"Learning rate: {lr}\")\n",
        "print(f\"Weight decay: {weight_decay}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Epochs: {n_epochs}\\n\")\n",
        "\n",
        "\n",
        "# ================== 訓練ループ ==================\n",
        "\n",
        "best_valid_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    train_num = 0\n",
        "    train_true_num = 0\n",
        "    valid_num = 0\n",
        "    valid_true_num = 0\n",
        "\n",
        "    mlp.train()\n",
        "    for x, t in dataloader_train:\n",
        "        x, t = x.to(device), t.to(device)\n",
        "\n",
        "        # データ拡張（テンソルスライスのみ使用）\n",
        "        x = augment_data(x, shift_range=2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 順伝播\n",
        "        y = mlp(x)\n",
        "\n",
        "        # 損失計算（クロスエントロピー）\n",
        "        log_softmax = torch.log(softmax(y) + 1e-7)\n",
        "        loss = -torch.mean(log_softmax[torch.arange(t.size(0), device=device), t])\n",
        "\n",
        "        # 逆伝播\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses_train.append(loss.item())\n",
        "\n",
        "        # 精度計算\n",
        "        pred = y.argmax(1)\n",
        "        acc = torch.where(t - pred == 0, torch.ones_like(t), torch.zeros_like(t))\n",
        "        train_num += acc.size(0)\n",
        "        train_true_num += acc.sum().item()\n",
        "\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, t in dataloader_valid:\n",
        "            x, t = x.to(device), t.to(device)\n",
        "\n",
        "            # 順伝播\n",
        "            y = mlp(x)\n",
        "\n",
        "            # 損失計算\n",
        "            log_softmax = torch.log(softmax(y) + 1e-7)\n",
        "            loss = -torch.mean(log_softmax[torch.arange(t.size(0), device=device), t])\n",
        "\n",
        "            losses_valid.append(loss.item())\n",
        "\n",
        "            # 精度計算\n",
        "            pred = y.argmax(1)\n",
        "            acc = torch.where(t - pred == 0, torch.ones_like(t), torch.zeros_like(t))\n",
        "            valid_num += acc.size(0)\n",
        "            valid_true_num += acc.sum().item()\n",
        "\n",
        "    # 学習率の更新\n",
        "    scheduler.step()\n",
        "\n",
        "    train_acc = train_true_num / train_num\n",
        "    valid_acc = valid_true_num / valid_num\n",
        "\n",
        "    # ベストモデルの保存\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model_state = mlp.state_dict()\n",
        "\n",
        "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}], LR: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        np.mean(losses_train),\n",
        "        train_acc,\n",
        "        np.mean(losses_valid),\n",
        "        valid_acc,\n",
        "        optimizer.param_groups[0]['lr']\n",
        "    ))\n",
        "\n",
        "# ベストモデルをロード\n",
        "mlp.load_state_dict(best_model_state)\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f} ({best_valid_acc*100:.2f}%)')\n",
        "print(f'{\"=\"*60}\\n')\n",
        "\n",
        "\n",
        "# ================== テストデータの予測 ==================\n",
        "\n",
        "mlp.eval()\n",
        "t_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x in dataloader_test:\n",
        "        x = x.to(device)\n",
        "\n",
        "        # 順伝播\n",
        "        y = mlp(x)\n",
        "\n",
        "        # モデルの出力を予測値のスカラーに変換\n",
        "        pred = y.argmax(1).tolist()\n",
        "        t_pred.extend(pred)\n",
        "\n",
        "submission = pd.Series(t_pred, name='label')\n",
        "submission.to_csv(work_dir + '/Lecture04/submission_pred_04.csv', header=True, index_label='id')\n",
        "\n",
        "print(f'Predictions saved to submission_pred_04.csv')\n",
        "print(f'Total test samples: {len(t_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bWurXw-zdDIy",
        "outputId": "e88c7afe-53a6-4c86-832d-33b9ff58987c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Train+Val samples: 60000, Test samples: 10000\n",
            "\n",
            "Model architecture:\n",
            "Hidden layers: [1024, 512, 256, 128]\n",
            "Dropout: 0.4\n",
            "Learning rate: 0.001\n",
            "Weight decay: 0.0001\n",
            "Batch size: 128\n",
            "Epochs: 400\n",
            "\n",
            "EPOCH: 1, Train [Loss: 0.990, Accuracy: 0.641], Valid [Loss: 0.578, Accuracy: 0.778], LR: 0.001000\n",
            "EPOCH: 2, Train [Loss: 0.711, Accuracy: 0.737], Valid [Loss: 0.522, Accuracy: 0.803], LR: 0.001000\n",
            "EPOCH: 3, Train [Loss: 0.646, Accuracy: 0.764], Valid [Loss: 0.474, Accuracy: 0.820], LR: 0.001000\n",
            "EPOCH: 4, Train [Loss: 0.613, Accuracy: 0.778], Valid [Loss: 0.471, Accuracy: 0.824], LR: 0.001000\n",
            "EPOCH: 5, Train [Loss: 0.589, Accuracy: 0.785], Valid [Loss: 0.449, Accuracy: 0.830], LR: 0.001000\n",
            "EPOCH: 6, Train [Loss: 0.569, Accuracy: 0.794], Valid [Loss: 0.439, Accuracy: 0.834], LR: 0.000999\n",
            "EPOCH: 7, Train [Loss: 0.559, Accuracy: 0.795], Valid [Loss: 0.430, Accuracy: 0.837], LR: 0.000999\n",
            "EPOCH: 8, Train [Loss: 0.546, Accuracy: 0.802], Valid [Loss: 0.430, Accuracy: 0.834], LR: 0.000999\n",
            "EPOCH: 9, Train [Loss: 0.543, Accuracy: 0.804], Valid [Loss: 0.399, Accuracy: 0.853], LR: 0.000999\n",
            "EPOCH: 10, Train [Loss: 0.530, Accuracy: 0.807], Valid [Loss: 0.421, Accuracy: 0.839], LR: 0.000998\n",
            "EPOCH: 11, Train [Loss: 0.528, Accuracy: 0.808], Valid [Loss: 0.397, Accuracy: 0.850], LR: 0.000998\n",
            "EPOCH: 12, Train [Loss: 0.522, Accuracy: 0.811], Valid [Loss: 0.400, Accuracy: 0.851], LR: 0.000998\n",
            "EPOCH: 13, Train [Loss: 0.516, Accuracy: 0.814], Valid [Loss: 0.415, Accuracy: 0.842], LR: 0.000997\n",
            "EPOCH: 14, Train [Loss: 0.511, Accuracy: 0.815], Valid [Loss: 0.385, Accuracy: 0.858], LR: 0.000997\n",
            "EPOCH: 15, Train [Loss: 0.507, Accuracy: 0.819], Valid [Loss: 0.386, Accuracy: 0.853], LR: 0.000997\n",
            "EPOCH: 16, Train [Loss: 0.501, Accuracy: 0.818], Valid [Loss: 0.380, Accuracy: 0.857], LR: 0.000996\n",
            "EPOCH: 17, Train [Loss: 0.498, Accuracy: 0.819], Valid [Loss: 0.382, Accuracy: 0.858], LR: 0.000996\n",
            "EPOCH: 18, Train [Loss: 0.499, Accuracy: 0.821], Valid [Loss: 0.375, Accuracy: 0.859], LR: 0.000995\n",
            "EPOCH: 19, Train [Loss: 0.493, Accuracy: 0.821], Valid [Loss: 0.372, Accuracy: 0.863], LR: 0.000994\n",
            "EPOCH: 20, Train [Loss: 0.491, Accuracy: 0.822], Valid [Loss: 0.377, Accuracy: 0.855], LR: 0.000994\n",
            "EPOCH: 21, Train [Loss: 0.489, Accuracy: 0.824], Valid [Loss: 0.364, Accuracy: 0.863], LR: 0.000993\n",
            "EPOCH: 22, Train [Loss: 0.489, Accuracy: 0.825], Valid [Loss: 0.367, Accuracy: 0.864], LR: 0.000993\n",
            "EPOCH: 23, Train [Loss: 0.487, Accuracy: 0.823], Valid [Loss: 0.387, Accuracy: 0.856], LR: 0.000992\n",
            "EPOCH: 24, Train [Loss: 0.484, Accuracy: 0.825], Valid [Loss: 0.368, Accuracy: 0.865], LR: 0.000991\n",
            "EPOCH: 25, Train [Loss: 0.481, Accuracy: 0.827], Valid [Loss: 0.354, Accuracy: 0.867], LR: 0.000990\n",
            "EPOCH: 26, Train [Loss: 0.483, Accuracy: 0.826], Valid [Loss: 0.361, Accuracy: 0.865], LR: 0.000990\n",
            "EPOCH: 27, Train [Loss: 0.479, Accuracy: 0.827], Valid [Loss: 0.355, Accuracy: 0.867], LR: 0.000989\n",
            "EPOCH: 28, Train [Loss: 0.479, Accuracy: 0.827], Valid [Loss: 0.371, Accuracy: 0.858], LR: 0.000988\n",
            "EPOCH: 29, Train [Loss: 0.473, Accuracy: 0.830], Valid [Loss: 0.352, Accuracy: 0.870], LR: 0.000987\n",
            "EPOCH: 30, Train [Loss: 0.476, Accuracy: 0.828], Valid [Loss: 0.369, Accuracy: 0.859], LR: 0.000986\n",
            "EPOCH: 31, Train [Loss: 0.469, Accuracy: 0.830], Valid [Loss: 0.354, Accuracy: 0.867], LR: 0.000985\n",
            "EPOCH: 32, Train [Loss: 0.469, Accuracy: 0.831], Valid [Loss: 0.375, Accuracy: 0.861], LR: 0.000984\n",
            "EPOCH: 33, Train [Loss: 0.472, Accuracy: 0.828], Valid [Loss: 0.357, Accuracy: 0.866], LR: 0.000983\n",
            "EPOCH: 34, Train [Loss: 0.468, Accuracy: 0.832], Valid [Loss: 0.340, Accuracy: 0.875], LR: 0.000982\n",
            "EPOCH: 35, Train [Loss: 0.469, Accuracy: 0.830], Valid [Loss: 0.348, Accuracy: 0.872], LR: 0.000981\n",
            "EPOCH: 36, Train [Loss: 0.469, Accuracy: 0.831], Valid [Loss: 0.358, Accuracy: 0.863], LR: 0.000980\n",
            "EPOCH: 37, Train [Loss: 0.472, Accuracy: 0.832], Valid [Loss: 0.366, Accuracy: 0.861], LR: 0.000979\n",
            "EPOCH: 38, Train [Loss: 0.464, Accuracy: 0.831], Valid [Loss: 0.357, Accuracy: 0.868], LR: 0.000978\n",
            "EPOCH: 39, Train [Loss: 0.467, Accuracy: 0.832], Valid [Loss: 0.351, Accuracy: 0.868], LR: 0.000977\n",
            "EPOCH: 40, Train [Loss: 0.464, Accuracy: 0.835], Valid [Loss: 0.344, Accuracy: 0.874], LR: 0.000976\n",
            "EPOCH: 41, Train [Loss: 0.465, Accuracy: 0.832], Valid [Loss: 0.350, Accuracy: 0.869], LR: 0.000974\n",
            "EPOCH: 42, Train [Loss: 0.464, Accuracy: 0.833], Valid [Loss: 0.359, Accuracy: 0.861], LR: 0.000973\n",
            "EPOCH: 43, Train [Loss: 0.462, Accuracy: 0.834], Valid [Loss: 0.352, Accuracy: 0.870], LR: 0.000972\n",
            "EPOCH: 44, Train [Loss: 0.457, Accuracy: 0.833], Valid [Loss: 0.340, Accuracy: 0.875], LR: 0.000970\n",
            "EPOCH: 45, Train [Loss: 0.462, Accuracy: 0.834], Valid [Loss: 0.336, Accuracy: 0.875], LR: 0.000969\n",
            "EPOCH: 46, Train [Loss: 0.459, Accuracy: 0.836], Valid [Loss: 0.345, Accuracy: 0.871], LR: 0.000968\n",
            "EPOCH: 47, Train [Loss: 0.467, Accuracy: 0.832], Valid [Loss: 0.341, Accuracy: 0.875], LR: 0.000966\n",
            "EPOCH: 48, Train [Loss: 0.461, Accuracy: 0.833], Valid [Loss: 0.347, Accuracy: 0.875], LR: 0.000965\n",
            "EPOCH: 49, Train [Loss: 0.457, Accuracy: 0.835], Valid [Loss: 0.339, Accuracy: 0.874], LR: 0.000963\n",
            "EPOCH: 50, Train [Loss: 0.462, Accuracy: 0.833], Valid [Loss: 0.361, Accuracy: 0.864], LR: 0.000962\n",
            "EPOCH: 51, Train [Loss: 0.455, Accuracy: 0.837], Valid [Loss: 0.353, Accuracy: 0.868], LR: 0.000960\n",
            "EPOCH: 52, Train [Loss: 0.456, Accuracy: 0.836], Valid [Loss: 0.336, Accuracy: 0.876], LR: 0.000959\n",
            "EPOCH: 53, Train [Loss: 0.462, Accuracy: 0.835], Valid [Loss: 0.339, Accuracy: 0.877], LR: 0.000957\n",
            "EPOCH: 54, Train [Loss: 0.460, Accuracy: 0.836], Valid [Loss: 0.334, Accuracy: 0.877], LR: 0.000956\n",
            "EPOCH: 55, Train [Loss: 0.459, Accuracy: 0.836], Valid [Loss: 0.359, Accuracy: 0.864], LR: 0.000954\n",
            "EPOCH: 56, Train [Loss: 0.455, Accuracy: 0.836], Valid [Loss: 0.350, Accuracy: 0.869], LR: 0.000952\n",
            "EPOCH: 57, Train [Loss: 0.458, Accuracy: 0.835], Valid [Loss: 0.348, Accuracy: 0.872], LR: 0.000951\n",
            "EPOCH: 58, Train [Loss: 0.456, Accuracy: 0.836], Valid [Loss: 0.339, Accuracy: 0.872], LR: 0.000949\n",
            "EPOCH: 59, Train [Loss: 0.458, Accuracy: 0.834], Valid [Loss: 0.336, Accuracy: 0.878], LR: 0.000947\n",
            "EPOCH: 60, Train [Loss: 0.456, Accuracy: 0.834], Valid [Loss: 0.347, Accuracy: 0.872], LR: 0.000946\n",
            "EPOCH: 61, Train [Loss: 0.453, Accuracy: 0.837], Valid [Loss: 0.341, Accuracy: 0.876], LR: 0.000944\n",
            "EPOCH: 62, Train [Loss: 0.454, Accuracy: 0.835], Valid [Loss: 0.354, Accuracy: 0.866], LR: 0.000942\n",
            "EPOCH: 63, Train [Loss: 0.452, Accuracy: 0.838], Valid [Loss: 0.340, Accuracy: 0.872], LR: 0.000940\n",
            "EPOCH: 64, Train [Loss: 0.453, Accuracy: 0.837], Valid [Loss: 0.345, Accuracy: 0.869], LR: 0.000938\n",
            "EPOCH: 65, Train [Loss: 0.453, Accuracy: 0.836], Valid [Loss: 0.325, Accuracy: 0.881], LR: 0.000936\n",
            "EPOCH: 66, Train [Loss: 0.452, Accuracy: 0.837], Valid [Loss: 0.338, Accuracy: 0.870], LR: 0.000934\n",
            "EPOCH: 67, Train [Loss: 0.451, Accuracy: 0.837], Valid [Loss: 0.334, Accuracy: 0.873], LR: 0.000932\n",
            "EPOCH: 68, Train [Loss: 0.453, Accuracy: 0.837], Valid [Loss: 0.335, Accuracy: 0.874], LR: 0.000930\n",
            "EPOCH: 69, Train [Loss: 0.448, Accuracy: 0.839], Valid [Loss: 0.328, Accuracy: 0.879], LR: 0.000928\n",
            "EPOCH: 70, Train [Loss: 0.452, Accuracy: 0.836], Valid [Loss: 0.326, Accuracy: 0.882], LR: 0.000926\n",
            "EPOCH: 71, Train [Loss: 0.450, Accuracy: 0.838], Valid [Loss: 0.362, Accuracy: 0.863], LR: 0.000924\n",
            "EPOCH: 72, Train [Loss: 0.451, Accuracy: 0.838], Valid [Loss: 0.345, Accuracy: 0.869], LR: 0.000922\n",
            "EPOCH: 73, Train [Loss: 0.452, Accuracy: 0.838], Valid [Loss: 0.344, Accuracy: 0.873], LR: 0.000920\n",
            "EPOCH: 74, Train [Loss: 0.447, Accuracy: 0.840], Valid [Loss: 0.345, Accuracy: 0.872], LR: 0.000918\n",
            "EPOCH: 75, Train [Loss: 0.453, Accuracy: 0.836], Valid [Loss: 0.331, Accuracy: 0.876], LR: 0.000916\n",
            "EPOCH: 76, Train [Loss: 0.445, Accuracy: 0.840], Valid [Loss: 0.339, Accuracy: 0.873], LR: 0.000914\n",
            "EPOCH: 77, Train [Loss: 0.448, Accuracy: 0.838], Valid [Loss: 0.332, Accuracy: 0.877], LR: 0.000911\n",
            "EPOCH: 78, Train [Loss: 0.447, Accuracy: 0.839], Valid [Loss: 0.335, Accuracy: 0.876], LR: 0.000909\n",
            "EPOCH: 79, Train [Loss: 0.445, Accuracy: 0.840], Valid [Loss: 0.341, Accuracy: 0.876], LR: 0.000907\n",
            "EPOCH: 80, Train [Loss: 0.449, Accuracy: 0.840], Valid [Loss: 0.353, Accuracy: 0.865], LR: 0.000905\n",
            "EPOCH: 81, Train [Loss: 0.445, Accuracy: 0.839], Valid [Loss: 0.333, Accuracy: 0.875], LR: 0.000902\n",
            "EPOCH: 82, Train [Loss: 0.452, Accuracy: 0.838], Valid [Loss: 0.335, Accuracy: 0.877], LR: 0.000900\n",
            "EPOCH: 83, Train [Loss: 0.446, Accuracy: 0.839], Valid [Loss: 0.331, Accuracy: 0.875], LR: 0.000898\n",
            "EPOCH: 84, Train [Loss: 0.443, Accuracy: 0.841], Valid [Loss: 0.331, Accuracy: 0.876], LR: 0.000895\n",
            "EPOCH: 85, Train [Loss: 0.446, Accuracy: 0.840], Valid [Loss: 0.328, Accuracy: 0.881], LR: 0.000893\n",
            "EPOCH: 86, Train [Loss: 0.445, Accuracy: 0.842], Valid [Loss: 0.322, Accuracy: 0.880], LR: 0.000890\n",
            "EPOCH: 87, Train [Loss: 0.441, Accuracy: 0.842], Valid [Loss: 0.336, Accuracy: 0.877], LR: 0.000888\n",
            "EPOCH: 88, Train [Loss: 0.446, Accuracy: 0.837], Valid [Loss: 0.326, Accuracy: 0.879], LR: 0.000885\n",
            "EPOCH: 89, Train [Loss: 0.444, Accuracy: 0.841], Valid [Loss: 0.338, Accuracy: 0.874], LR: 0.000883\n",
            "EPOCH: 90, Train [Loss: 0.443, Accuracy: 0.841], Valid [Loss: 0.331, Accuracy: 0.876], LR: 0.000880\n",
            "EPOCH: 91, Train [Loss: 0.445, Accuracy: 0.839], Valid [Loss: 0.328, Accuracy: 0.877], LR: 0.000878\n",
            "EPOCH: 92, Train [Loss: 0.443, Accuracy: 0.840], Valid [Loss: 0.331, Accuracy: 0.875], LR: 0.000875\n",
            "EPOCH: 93, Train [Loss: 0.445, Accuracy: 0.840], Valid [Loss: 0.326, Accuracy: 0.875], LR: 0.000873\n",
            "EPOCH: 94, Train [Loss: 0.443, Accuracy: 0.842], Valid [Loss: 0.335, Accuracy: 0.875], LR: 0.000870\n",
            "EPOCH: 95, Train [Loss: 0.446, Accuracy: 0.841], Valid [Loss: 0.333, Accuracy: 0.876], LR: 0.000867\n",
            "EPOCH: 96, Train [Loss: 0.440, Accuracy: 0.842], Valid [Loss: 0.336, Accuracy: 0.877], LR: 0.000865\n",
            "EPOCH: 97, Train [Loss: 0.441, Accuracy: 0.842], Valid [Loss: 0.332, Accuracy: 0.875], LR: 0.000862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _releaseLock at 0x7b54b2da56c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
            "    def _releaseLock():\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1278593390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}